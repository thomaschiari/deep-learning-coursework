{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#deep-learning-coursework","title":"Deep Learning Coursework","text":"<p>Author: Thomas Chiari Ciocchetti de Souza</p> <p>Group Members:</p> <ul> <li> <p>Felipe Maluli</p> </li> <li> <p>Lucca Hiratsuca</p> </li> <li> <p>Thomas Chiari Ciocchetti de Souza</p> </li> </ul>"},{"location":"#deliverables","title":"Deliverables","text":"<ul> <li> <p> Data Task - 05/09/2025</p> </li> <li> <p> Perceptron Task - 14/09/2025</p> </li> <li> <p> MLP Task - 24/09/2025</p> </li> <li> <p> Classification Project - 05/10/2025</p> </li> <li> <p> VAE Task - 26/10/2025</p> </li> <li> <p> Regression Project - 26/10/2025</p> </li> </ul>"},{"location":"#references","title":"References","text":"<p>Material for MkDocs</p>"},{"location":"classification-project/classification-project/","title":"Classification Project","text":""},{"location":"classification-project/classification-project/#classification-project","title":"Classification Project","text":"<p>The complete source code for this project is available here.</p>"},{"location":"classification-project/classification-project/#1-dataset-selection","title":"1. Dataset Selection","text":"<p>Name: Bank Marketing Dataset</p> <p>Source: UCI Machine Learning Repository available here</p> <p>Size: 41188 rows, with 20 features and 1 target variable</p> <p>Task: Binary classification, predicting whether a client will subscribe to a term deposit</p> <p>Why this dataset? </p> <p>We wanted a real-world, business-relevant classification problem with enough rows and feature diversity to justify an MLP. The Bank Marketing dataset provides 41k examples with both categorical and numeric attributes (client profile + campaign context), enabling preprocessing (one-hot + scaling) and meaningful model comparisons. The target is notably imbalanced (~89% \u201cno\u201d, ~11% \u201cyes\u201d), making it more complex than a balanced dataset. The dataset also has no missing values, and is a public academic dataset.</p>"},{"location":"classification-project/classification-project/#2-dataset-explanation","title":"2. Dataset Explanation","text":"<p>The dataset represents records from telemarketing campaigns of a Portuguese bank. The goal is to predict whether the client will subscribe to a term deposit. The file <code>bank-additional-full.csv</code> contains 41,188 examples and 20 inputs. There are no missing values.</p> <p>The target is column <code>y</code>. It is a binary variable, indicating whether the client will subscribe to a term deposit, and is imbalanced (~89% \u201cno\u201d, ~11% \u201cyes\u201d).</p> <p>Features:</p> <ul> <li><code>age</code>: numeric</li> </ul> <p></p> <ul> <li><code>job</code>: categorical, indicating the type of job of the client</li> </ul> <p></p> <ul> <li><code>marital</code>: categorical, indicating the marital status of the client</li> </ul> <p></p> <ul> <li><code>education</code>: categorical, indicating the education level of the client</li> </ul> <p></p> <ul> <li><code>default</code>: categorical, indicating if the client has credit in default</li> </ul> <p></p> <ul> <li><code>housing</code>: categorical, indicating if the client has a housing loan</li> </ul> <p></p> <ul> <li><code>loan</code>: categorical, indicating if the client has a personal loan</li> </ul> <p></p> <ul> <li><code>contact</code>: categorical, indicating the communication type (e.g. telephone, cellular...)</li> </ul> <p></p> <ul> <li><code>month</code>: categorical, indicating the last contact month of the year</li> </ul> <p></p> <ul> <li><code>day_of_week</code>: categorical, indicating the last contact day of the week</li> </ul> <p></p> <ul> <li> <p><code>duration</code>: numeric, indicating the last call duration in seconds (this feature is dropped because it leaks the outcome, e.g. <code>duration=0</code> \u2192 always <code>y=\"no\"</code>)</p> </li> <li> <p><code>campaign</code>: numeric, indicating the number of contacts in the current campaign</p> </li> </ul> <p></p> <ul> <li><code>pdays</code>: numeric, indicating the number of days that passed by after the client was last contacted (999 means not previously contacted)</li> </ul> <p></p> <ul> <li><code>previous</code>: numeric, indicating the number of contacts before the current campaign</li> </ul> <p></p> <ul> <li><code>poutcome</code>: categorical, indicating the outcome of the previous marketing campaign</li> </ul> <p></p> <ul> <li><code>emp.var.rate</code>: numeric, indicating the employment variation rate</li> </ul> <p></p> <ul> <li><code>cons.price.idx</code>: numeric, indicating the consumer price index</li> </ul> <p></p> <ul> <li><code>cons.conf.idx</code>: numeric, indicating the consumer confidence index</li> </ul> <p></p> <ul> <li><code>euribor3m</code>: numeric, indicating the euribor 3 month rate</li> </ul> <p></p> <ul> <li><code>nr.employed</code>: numeric, indicating the number of employees</li> </ul> <p></p> <p>The last 5 features are macroeconomic indicators, important for context.</p> <p>Potential issues to address</p> <ul> <li>Imbalance: the target is imbalanced (~89% \u201cno\u201d, ~11% \u201cyes\u201d).</li> </ul> <p>Summary statistics and visuals</p> <p>Class distribution of the target:</p> <p></p> <p>Correlation matrix of the numeric features:</p> <p></p> <p>Summary statistics of the numeric features:</p> <p></p> <p>We additionally created a new feature <code>prev_contacted</code>, indicating if the client was previously contacted by treating the <code>pdays</code> feature as 1 if the client was previously contacted, and 0 otherwise.</p>"},{"location":"classification-project/classification-project/#3-data-preprocessing","title":"3. Data Preprocessing","text":""},{"location":"classification-project/classification-project/#treating-missing-values-and-duplicates","title":"Treating missing values and duplicates","text":"<p>The first step in order to clean and preprocess the data was to treat the variable <code>pdays</code> for when there was no previous contact. For that, we used the created <code>prev_contacted</code> feature to filter the rows where the client was not previously contacted, then imputed the median value of <code>pdays</code>. This is the only feature with \"missing\" values, and we chose to fill with the median value to avoid introducing new labels.</p> <pre><code>df.loc[df[\"prev_contacted\"] == 0, \"pdays\"] = np.nan\n\nimputer = SimpleImputer(strategy='median')\ndf[numeric_features] = imputer.fit_transform(df[numeric_features])\n</code></pre> <p>The next step was to drop the duplicates. for that, we used the <code>drop_duplicates</code> function: </p> <pre><code>before_n = len(df)\ndf = df.drop_duplicates()\nafter_n = len(df)\ndedup_removed = before_n - after_n\ndedup_removed\n</code></pre> <p>With that, we removed 1784 rows that were exactly the same. </p>"},{"location":"classification-project/classification-project/#scaling-numerical-features","title":"Scaling numerical features","text":"<p>The next step was to scale numerical features. We applied standardization (z-score), which centers features around 0 with standard deviation of 1. This was made in order to remove the effect of different scales between features, equalizing variances, making the data robust to outliers. Min-max scaling was also considered, but it is highly sensitive to outliers. A few extremes may squash most data into a narrow range. </p> <pre><code>for c in numeric_features:\n    df[c] = pd.to_numeric(df[c], errors='coerce')\n\nscaler = StandardScaler()\ndf[numeric_features] = scaler.fit_transform(df[numeric_features])\n</code></pre> <p>With that, we keep the distribution of the data, but the values are normalized relative to other features. Here is a histogram of the <code>age</code> feature after scaling:</p> <p></p>"},{"location":"classification-project/classification-project/#encoding-categorical-features","title":"Encoding categorical features","text":"<p>We used one-hot encoding for all categorical features. We kept all information, avoiding the creation of a reference level. This is because neural networks are not sensitive to multicollinearity in the same way as linear models. We chose one-hot because it encodes each category as a binary feature, avoiding the creation of an order between the categories.</p> <pre><code>X_cat = pd.get_dummies(df[categorical_features], drop_first=False, dtype=np.float32)\n\nprocessed = pd.concat(\n    [df[numeric_features].astype(np.float32), X_cat],\n    axis=1,\n)\n\nprocessed.insert(0, \"y\", df[\"y\"].astype(np.int8))\n</code></pre> <p>Finally, we saved the processed data into a CSV file in order to use it as input for the MLP in the next step.</p>"},{"location":"classification-project/classification-project/#4-mlp-implementation","title":"4. MLP Implementation","text":"<p>Now, we are going to implement a multi-layer perceptron (MLP) from scratch using Numpy operations, trained with mini-batch SGD and cross-entropy. The model supports an arbitrary number of hidden layers selectable via CLI (argument parser), and supports <code>relu</code>, <code>tanh</code> and <code>sigmoid</code> activations. </p> <p>We train on the data that was the output of the previous step, where the first column is the target and all remaining columns are the treated features. We use a split of 70% for training, 15% for validation and 15% for testing in the training loop. </p> <p>How to run the MLP (in the source code repository):</p> <pre><code>python src/mlp_numpy.py --data data/clean/bank-additional-full-post-preprocessed.csv --hidden 64,64 --activation relu --epochs 30 --lr 0.05 --batch_size 256 --seed 42\n</code></pre>"},{"location":"classification-project/classification-project/#hyperparameters","title":"Hyperparameters","text":"<ul> <li> <p>Hidden Layers: selectable via CLI (size of each layer). Default is <code>64</code>.</p> </li> <li> <p>Activation Functions: selectable via CLI (either <code>relu</code>, <code>tanh</code> or <code>sigmoid</code>). Default is <code>relu</code>.</p> </li> <li> <p>Epochs: selectable via CLI (number of epochs). Default is <code>30</code>.</p> </li> <li> <p>Learning Rate: selectable via CLI (learning rate). Default is <code>0.05</code>.</p> </li> <li> <p>Batch Size: selectable via CLI (batch size). Default is <code>256</code>.</p> </li> <li> <p>Seed: selectable via CLI (random seed). Default is <code>42</code>.</p> </li> </ul>"},{"location":"classification-project/classification-project/#architecture","title":"Architecture","text":"<p>The model is a feed-forward network: input -&gt; hidden layers -&gt; output logits -&gt; softmax. Here is the initial implementation of the MLP class:</p> <pre><code>class MLP:\n    def __init__(self, input_dim, hidden_layers, num_classes, activation=\"relu\", seed=42, l2=0.0):\n        self.activation = activation\n        self.l2 = float(l2)\n        rng = np.random.default_rng(seed)\n\n        sizes = [input_dim] + list(hidden_layers) + [num_classes]\n        self.W = []\n        self.b = []\n        for i in range(len(sizes)-1):\n            fan_in, fan_out = sizes[i], sizes[i+1]\n            if activation == \"relu\":\n                W = rng.normal(0.0, np.sqrt(2.0/fan_in), size=(fan_in, fan_out)).astype(np.float32)\n            else:  \n                W = rng.normal(0.0, np.sqrt(1.0/fan_in), size=(fan_in, fan_out)).astype(np.float32)\n            b = np.zeros((1, fan_out), dtype=np.float32)\n            self.W.append(W); self.b.append(b)\n</code></pre>"},{"location":"classification-project/classification-project/#activation-functions","title":"Activation functions","text":"<p>Activation functions introduce non-linearity to the model, allowing it to learn non-linear boundaries. We support three activation functions: <code>relu</code>, <code>tanh</code> and <code>sigmoid</code>. Here is the implementation of the activation functions:</p> <pre><code># activation functions\ndef act_forward(z, kind):\n    if kind == \"relu\":   \n        return np.maximum(0, z)\n    if kind == \"tanh\":   \n        return np.tanh(z)\n    if kind == \"sigmoid\":\n        return 1.0 / (1.0 + np.exp(-z))\n    raise ValueError(\"activation must be relu/tanh/sigmoid\")\n\ndef act_backward(z, a, kind):\n    if kind == \"relu\":    \n        return (z &gt; 0).astype(z.dtype)\n    if kind == \"tanh\":    \n        return 1.0 - a*a\n    if kind == \"sigmoid\": \n        return a * (1.0 - a)\n    raise ValueError(\"activation must be relu/tanh/sigmoid\")\n</code></pre> <p>The forward pass per layer is implemented as follows:</p> <pre><code>def forward(self, X):\n    \"\"\"Returns probs and caches for backprop\"\"\"\n    A = X\n    caches = []  \n    L = len(self.W)\n    for l in range(L):\n        Z = A @ self.W[l] + self.b[l]   \n        if l &lt; L-1:\n            A_next = act_forward(Z, self.activation)\n        else:\n            A_next = Z\n        caches.append((A, Z, A_next))\n        A = A_next\n    probs = softmax(A)  \n    return probs, caches\n</code></pre> <p>And the softmax function, for the output layer, is implemented as follows:</p> <pre><code>def softmax(z):\n    z = z - z.max(axis=1, keepdims=True) \n    ez = np.exp(z)\n    return ez / (ez.sum(axis=1, keepdims=True) + 1e-12)\n</code></pre>"},{"location":"classification-project/classification-project/#loss-function","title":"Loss function","text":"<p>The loss function is the cross-entropy on the softmax probabilities, calculated as follows:</p> <pre><code>tr_loss = -np.log(tr_probs[np.arange(Xtr.shape[0]), ytr] + 1e-12).mean()\nva_loss = -np.log(va_probs[np.arange(Xva.shape[0]), yva] + 1e-12).mean()\n</code></pre>"},{"location":"classification-project/classification-project/#optimizer","title":"Optimizer","text":"<p>The optimizer is a mini-batch SGD: we shuffle the data, take batches, compute the gradients, backpropagate and update the weights. Here is the implementation of the complete training loop:</p> <pre><code># train loop\ndef train(model, Xtr, ytr, Xva, yva, epochs=30, lr=0.05, batch_size=256, seed=42):\n    rng = np.random.default_rng(seed)\n    n = Xtr.shape[0]\n    for epoch in range(1, epochs+1):\n        # mini-batch SGD\n        idx = rng.permutation(n)\n        for start in range(0, n, batch_size):\n            b = idx[start:start+batch_size]\n            probs, caches = model.forward(Xtr[b])\n            dW, db = model.backward(probs, ytr[b], caches)\n            model.step(dW, db, lr)\n        # metrics\n        tr_pred, tr_probs = model.predict(Xtr)\n        va_pred, va_probs = model.predict(Xva)\n        tr_acc = accuracy(ytr, tr_pred)\n        va_acc = accuracy(yva, va_pred)\n        tr_loss = -np.log(tr_probs[np.arange(Xtr.shape[0]), ytr] + 1e-12).mean()\n        va_loss = -np.log(va_probs[np.arange(Xva.shape[0]), yva] + 1e-12).mean()\n        print(f\"epoch {epoch:03d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n</code></pre>"},{"location":"classification-project/classification-project/#final-model-implementation","title":"Final model implementation","text":"<p>The final model includes a CLI to parse the arguments, load the processed CSV, make the stratification, build the model, train, and evaluate on test:</p> <pre><code>import argparse\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n# activation functions\ndef act_forward(z, kind):\n    if kind == \"relu\":   \n        return np.maximum(0, z)\n    if kind == \"tanh\":   \n        return np.tanh(z)\n    if kind == \"sigmoid\":\n        return 1.0 / (1.0 + np.exp(-z))\n    raise ValueError(\"activation must be relu/tanh/sigmoid\")\n\ndef act_backward(z, a, kind):\n    if kind == \"relu\":    \n        return (z &gt; 0).astype(z.dtype)\n    if kind == \"tanh\":    \n        return 1.0 - a*a\n    if kind == \"sigmoid\": \n        return a * (1.0 - a)\n    raise ValueError(\"activation must be relu/tanh/sigmoid\")\n\n# helpers\ndef parse_hidden(s: str):\n    return [int(x) for x in s.split(\",\")] if s else [64]\n\ndef softmax(z):\n    z = z - z.max(axis=1, keepdims=True) \n    ez = np.exp(z)\n    return ez / (ez.sum(axis=1, keepdims=True) + 1e-12)\n\ndef accuracy(y_true, y_pred):\n    return float((y_true == y_pred).mean())\n\ndef stratified_split(X, y, train=0.70, val=0.15, seed=42):\n    rng = np.random.default_rng(seed)\n    classes = np.unique(y)\n    idx_tr, idx_va, idx_te = [], [], []\n    for c in classes:\n        idx = np.where(y == c)[0]\n        rng.shuffle(idx)\n        n = len(idx)\n        n_tr = int(n * train)\n        n_va = int(n * val)\n        idx_tr.append(idx[:n_tr])\n        idx_va.append(idx[n_tr:n_tr+n_va])\n        idx_te.append(idx[n_tr+n_va:])\n    idx_tr = np.concatenate(idx_tr); idx_va = np.concatenate(idx_va); idx_te = np.concatenate(idx_te)\n    rng.shuffle(idx_tr); rng.shuffle(idx_va); rng.shuffle(idx_te)\n    return (X[idx_tr], y[idx_tr]), (X[idx_va], y[idx_va]), (X[idx_te], y[idx_te])\n\n\n# mlp class\nclass MLP:\n    def __init__(self, input_dim, hidden_layers, num_classes, activation=\"relu\", seed=42, l2=0.0):\n        self.activation = activation\n        self.l2 = float(l2)\n        rng = np.random.default_rng(seed)\n\n        sizes = [input_dim] + list(hidden_layers) + [num_classes]\n        self.W = []\n        self.b = []\n        for i in range(len(sizes)-1):\n            fan_in, fan_out = sizes[i], sizes[i+1]\n            if activation == \"relu\":\n                W = rng.normal(0.0, np.sqrt(2.0/fan_in), size=(fan_in, fan_out)).astype(np.float32)\n            else:  \n                W = rng.normal(0.0, np.sqrt(1.0/fan_in), size=(fan_in, fan_out)).astype(np.float32)\n            b = np.zeros((1, fan_out), dtype=np.float32)\n            self.W.append(W); self.b.append(b)\n\n    def forward(self, X):\n        \"\"\"Returns probs and caches for backprop\"\"\"\n        A = X\n        caches = []  \n        L = len(self.W)\n        for l in range(L):\n            Z = A @ self.W[l] + self.b[l]   \n            if l &lt; L-1:\n                A_next = act_forward(Z, self.activation)\n            else:\n                A_next = Z\n            caches.append((A, Z, A_next))\n            A = A_next\n        probs = softmax(A)  \n        return probs, caches\n\n    def backward(self, probs, y, caches):\n        \"\"\"Cross-entropy grads; returns dW, db lists\"\"\"\n        N = y.shape[0]\n        L = len(self.W)\n        dZ = probs.copy()\n        dZ[np.arange(N), y] -= 1.0\n        dZ /= N\n\n        dW_list, db_list = [None]*L, [None]*L\n        for l in reversed(range(L)):\n            A_prev, Z, A = caches[l]\n            dW = A_prev.T @ dZ + self.l2 * self.W[l]\n            db = dZ.sum(axis=0, keepdims=True)\n            dW_list[l] = dW.astype(np.float32)\n            db_list[l] = db.astype(np.float32)\n\n            if l &gt; 0:\n                dA_prev = dZ @ self.W[l].T\n                A_prev_prev, Z_prev, A_prev_post = caches[l-1]\n                dZ = dA_prev * act_backward(Z_prev, A_prev_post, self.activation)\n        return dW_list, db_list\n\n    def step(self, dW_list, db_list, lr):\n        for l in range(len(self.W)):\n            self.W[l] -= lr * dW_list[l]\n            self.b[l] -= lr * db_list[l]\n\n    def predict(self, X):\n        probs, _ = self.forward(X)\n        return probs.argmax(axis=1), probs\n\n# train loop\ndef train(model, Xtr, ytr, Xva, yva, epochs=30, lr=0.05, batch_size=256, seed=42):\n    rng = np.random.default_rng(seed)\n    n = Xtr.shape[0]\n    for epoch in range(1, epochs+1):\n        # mini-batch SGD\n        idx = rng.permutation(n)\n        for start in range(0, n, batch_size):\n            b = idx[start:start+batch_size]\n            probs, caches = model.forward(Xtr[b])\n            dW, db = model.backward(probs, ytr[b], caches)\n            model.step(dW, db, lr)\n        # metrics\n        tr_pred, tr_probs = model.predict(Xtr)\n        va_pred, va_probs = model.predict(Xva)\n        tr_acc = accuracy(ytr, tr_pred)\n        va_acc = accuracy(yva, va_pred)\n        tr_loss = -np.log(tr_probs[np.arange(Xtr.shape[0]), ytr] + 1e-12).mean()\n        va_loss = -np.log(va_probs[np.arange(Xva.shape[0]), yva] + 1e-12).mean()\n        print(f\"epoch {epoch:03d} | train loss {tr_loss:.4f} acc {tr_acc:.4f} | val loss {va_loss:.4f} acc {va_acc:.4f}\")\n\n# main function with arg parse\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--data\", type=str, default=\"data/processed/processed.csv\")\n    ap.add_argument(\"--hidden\", type=str, default=\"64\", help='e.g. \"64\" or \"128,64,32\"')\n    ap.add_argument(\"--activation\", type=str, default=\"relu\", choices=[\"relu\",\"tanh\",\"sigmoid\"])\n    ap.add_argument(\"--epochs\", type=int, default=30)\n    ap.add_argument(\"--lr\", type=float, default=0.05)\n    ap.add_argument(\"--batch_size\", type=int, default=256)\n    ap.add_argument(\"--val_ratio\", type=float, default=0.15)\n    ap.add_argument(\"--test_ratio\", type=float, default=0.15)\n    ap.add_argument(\"--seed\", type=int, default=42)\n    ap.add_argument(\"--l2\", type=float, default=0.0)\n    args = ap.parse_args()\n\n    df = pd.read_csv(args.data)\n    y = df.iloc[:, 0].to_numpy(dtype=np.int64)\n    X = df.iloc[:, 1:].to_numpy(dtype=np.float32)\n\n    uniq, y_mapped = np.unique(y, return_inverse=True)\n    y = y_mapped.astype(np.int64)\n    num_classes = len(uniq)\n\n    tr, va, te = stratified_split(X, y, train=1.0-args.val_ratio-args.test_ratio, val=args.val_ratio, seed=args.seed)\n    (Xtr, ytr), (Xva, yva), (Xte, yte) = tr, va, te\n\n    model = MLP(\n        input_dim=Xtr.shape[1],\n        hidden_layers=parse_hidden(args.hidden),\n        num_classes=num_classes,\n        activation=args.activation,\n        seed=args.seed,\n        l2=args.l2\n    )\n\n    # train\n    train(model, Xtr, ytr, Xva, yva, epochs=args.epochs, lr=args.lr, batch_size=args.batch_size, seed=args.seed)\n\n    # test\n    yhat, _ = model.predict(Xte)\n    test_acc = accuracy(yte, yhat)\n    print(f\"\\nTest accuracy: {test_acc:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"classification-project/classification-project/#summary","title":"Summary","text":"<p>In this step, the MLP was implemented using only Numpy in a flexible way, so that a user can select the hyperparameters of the model, the architecture, activation functions, etc. This completes the implementation of the core model, which will be evaluated in the next section. </p> <p>Note: Artificial Intelligence was used in this exercise for code completion and review, as well as for text revision and refinement.</p>"},{"location":"classification-project/classification-project/#5-model-training","title":"5. Model Training","text":"<p>We train the NumPy MLP using mini\u2011batch SGD and cross\u2011entropy. The training loop implements: forward propagation \u2192 loss calculation \u2192 backpropagation \u2192 parameter updates. We initialize weights randomly (small variance) and optionally apply L2 penalty to reduce overfitting.</p> <p>Python (core idea): </p><pre><code>from mlp_numpy import MLP, stratified_split\nimport numpy as np, pandas as pd\n\ndf = pd.read_csv(\"src/data/clean/bank-additional-full-post-preprocessed.csv\")\ny = df.iloc[:,0].to_numpy(dtype=np.int64)\nX = df.iloc[:,1:].to_numpy(dtype=np.float32)\n\n(Xtr,ytr), (Xva,yva), (Xte,yte) = stratified_split(X, y, train=0.70, val=0.15, seed=42)\nmodel = MLP(input_dim=X.shape[1], hidden_layers=[128,64], num_classes=len(np.unique(y)),\n            activation=\"relu\", l2=1e-4, seed=42)\n# training loop: forward -&gt; loss -&gt; backward -&gt; step\n</code></pre> CLI (reproducible baseline): <pre><code>python src/step5_train_mlp.py --data src/data/clean/bank-additional-full-post-preprocessed.csv --hidden 128,64 --activation relu --epochs 60 --lr 0.03 --batch_size 512 --l2 1e-4 --seed 42\n</code></pre> Challenges &amp; fixes. To avoid saturation/vanishing gradients we prefer ReLU over <code>tanh/sigmoid</code> for hidden layers; we keep inputs standardized; and we use early stopping (below) plus mild L2 to control overfitting.<p></p>"},{"location":"classification-project/classification-project/#6-training-testing-strategy","title":"6. Training &amp; Testing Strategy","text":"<ul> <li>Split: stratified 70/15/15 (train/validation/test) with seed 42 for reproducibility.  </li> <li>Training mode: mini\u2011batch (batch size 512) balances speed and stability.  </li> <li>Early stopping: on validation loss with <code>patience=10</code>, <code>min_delta=1e-4</code>.  </li> <li>Rationale: Validation guides hyperparameter tuning (hidden sizes, LR, patience, L2).</li> </ul> <p>CLI (final early\u2011stopped run): </p><pre><code>python src/step6_strategy.py --data src/data/clean/bank-additional-full-post-preprocessed.csv --hidden 256,128,64 --activation relu --epochs 80 --lr 0.025 --batch_size 512 --patience 10 --min_delta 1e-4 --l2 1e-4 --seed 123\n</code></pre> Curves (deep/wide run): <p></p>"},{"location":"classification-project/classification-project/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":"<p>We show four plots to analyze convergence and generalization: training/validation loss and accuracy for a quick baseline and for the final deep/wide model.</p> <p>Notebook: <code>src/step7_curves.ipynb</code></p> <p>Quick baseline (32\u00d732) </p> <p>Deep/Wide (256\u00d7128\u00d764) </p> <p>Interpretation. The training curves reveal several important patterns:</p> <p>Early Training Phase (Epochs 1-20): Both training and validation metrics improve steadily, with validation accuracy actually exceeding training accuracy initially. This suggests the model is learning generalizable patterns effectively.</p> <p>Peak Performance (Epoch 21): Validation accuracy reaches its maximum at 0.9024 (epoch 21), indicating the optimal point before overfitting begins. The validation loss also reaches its minimum around this time.</p> <p>Overfitting Transition (Epochs 30-50): Around epoch 30, we observe a subtle but important shift: - Validation accuracy plateaus and begins fluctuating around 0.900-0.902, while training accuracy continues improving (reaching 0.900+ by epoch 45) - The accuracy gap reverses: Initially validation &gt; training (negative gap), but by epoch 45-50, training &gt; validation (positive gap) - Loss behavior mirrors this pattern: Validation loss initially lower than training loss, but the gap narrows and eventually reverses</p> <p>Late Training Phase (Epochs 50+): Clear overfitting emerges as training accuracy continues rising while validation accuracy stagnates or slightly declines. The model becomes increasingly specialized to the training set.</p> <p>Early Stopping Effectiveness: The model stopped at epoch 65 with the best validation performance at epoch 55, demonstrating that early stopping successfully prevented severe overfitting. The final model maintains good generalization despite the overfitting trend in later epochs.</p> <p>Key Insights: - Standardized inputs and L2 regularization (\u03bb=1e-4) help control overfitting but don't eliminate it entirely - The deep architecture (256\u00d7128\u00d764) has sufficient capacity to overfit, but early stopping provides effective regularization - The validation set effectively guides hyperparameter selection and prevents overfitting</p>"},{"location":"classification-project/classification-project/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":"<p>Notebook: <code>src/step8_metrics.ipynb</code></p> <p>We evaluate on the held\u2011out test set and compare to a majority\u2011class baseline. Because the dataset is imbalanced, we report ROC\u2011AUC and PR\u2011AUC (in addition to accuracy). We include a confusion matrix heatmap and the per\u2011class precision/recall/F1 table.</p>"},{"location":"classification-project/classification-project/#summary-metrics","title":"Summary metrics:","text":"Metric Value Interpretation Accuracy 0.897 Overall 89.7 % correct predictions; inflated by majority class Baseline Accuracy 0.883 Always predicting 0 (majority) would already get 88.3 % \u2014 so accuracy alone isn\u2019t very informative ROC\u2013AUC 0.790 Good discrimination ability across thresholds PR\u2013AUC 0.458 Moderate; model can identify some positives better than random"},{"location":"classification-project/classification-project/#confusion-matrix","title":"Confusion Matrix","text":"True Class \u2193 / Predicted \u2192 0 1 0 (negative) 5121 101 1 (positive) 510 181 <p>Interpretation:</p> <ul> <li>True Negatives (TN): 5121   \u2192 The model correctly predicted class 0 (negative) for 5121 samples.</li> <li>False Positives (FP): 101   \u2192 101 samples were incorrectly classified as positive.</li> <li>False Negatives (FN): 510   \u2192 510 actual positives were missed (classified as 0).</li> <li>True Positives (TP): 181   \u2192 181 actual positives were correctly detected.</li> </ul> <p>So while the overall accuracy is high (0.897), that\u2019s largely because class 0 dominates \u2014 the model is very good at recognizing negatives but struggles with positives.</p>"},{"location":"classification-project/classification-project/#roc-curve","title":"ROC Curve","text":"<p>The Receiver Operating Characteristic (ROC Curve (AUC = 0.790)) curve plots:</p> <ul> <li>x-axis: False Positive Rate (1 \u2212 Specificity)</li> <li>y-axis: True Positive Rate (Recall or Sensitivity)</li> </ul> <p>The AUC (Area Under Curve) measures how well the model separates the two classes regardless of threshold.</p> <p>Interpretation:</p> <ul> <li>AUC = 0.5: random guessing</li> <li>AUC = 1.0: perfect separation</li> <li>Your AUC = 0.79: good \u2014 the model can discriminate between classes better than random, but not perfectly.   In practical terms, given one random positive and one random negative, the model assigns the positive a higher score ~79 % of the time.</li> </ul> <p>The curve shape (steep rise near the origin, then tapering) suggests it achieves high recall with relatively few false positives early on, then saturates as threshold decreases.</p>"},{"location":"classification-project/classification-project/#precisionrecall-curve","title":"Precision\u2013Recall Curve","text":"<p>This plot is more informative for imbalanced datasets like yours.</p> <ul> <li>Precision: TP / (TP + FP) \u2014 how many predicted positives are correct.</li> <li>Recall: TP / (TP + FN) \u2014 how many actual positives are detected.</li> <li>AP (Average Precision): area under the Precision\u2013Recall curve (similar to AUC).</li> </ul> <p>Interpretation:</p> <ul> <li>Baseline (dotted line) represents the positive class prevalence.   So the model\u2019s precision is substantially above random across much of the curve.</li> <li>AP = 0.458 indicates moderate ability to rank positive cases above negatives, but still many false positives when recall increases.</li> <li>The sharp initial peaks show that at stricter thresholds, the model can achieve very high precision (but only for a small number of cases).</li> </ul>"},{"location":"classification-project/classification-project/#perclass-metrics","title":"Per\u2011class metrics:","text":"Class Precision Recall F1-score Support 0 0.909 0.981 0.944 5222 1 0.642 0.262 0.372 691 macro avg 0.776 0.621 0.658 5913 weighted avg 0.878 0.897 0.877 5913"},{"location":"classification-project/classification-project/#interpretation","title":"Interpretation","text":""},{"location":"classification-project/classification-project/#class-0-majority","title":"Class 0 (majority)","text":"<ul> <li>Precision = 0.909 \u2192 About 91 % of predicted 0\u2019s are truly 0.</li> <li>Recall = 0.981 \u2192 The model correctly finds almost all real 0\u2019s (misses only ~2 %).</li> <li>F1 = 0.944 \u2192 Excellent overall balance between precision and recall.</li> <li>Interpretation: The classifier is very reliable when identifying class 0.</li> </ul>"},{"location":"classification-project/classification-project/#class-1-minority","title":"Class 1 (minority)","text":"<ul> <li>Precision = 0.642 \u2192 Roughly two-thirds of predicted 1\u2019s are correct.</li> <li>Recall = 0.262 \u2192 Detects only about a quarter of the actual positives.</li> <li>F1 = 0.372 \u2192 Weak combined performance, driven by low recall.</li> <li>Interpretation: The model struggles to capture true positives, producing many false negatives.</li> </ul>"},{"location":"classification-project/classification-project/#macro-averages-unweighted","title":"Macro averages (unweighted)","text":"<ul> <li>Precision = 0.776, Recall = 0.621, F1 = 0.658</li> <li>Treats both classes equally.</li> <li>Shows moderate overall discrimination but clear imbalance between the two classes.</li> </ul>"},{"location":"classification-project/classification-project/#weighted-averages-by-class-frequency","title":"Weighted averages (by class frequency)","text":"<ul> <li>Precision = 0.878, Recall = 0.897, F1 = 0.877</li> <li>Dominated by class 0 performance.</li> <li>Closely matches overall test accuracy (~0.897).</li> </ul> <p>Notes. As typical in imbalanced settings, recall for the positive class is lower at default threshold 0.5; threshold tuning, class weighting, or cost\u2011sensitive optimization can raise recall for the positive class with acceptable precision trade\u2011offs.</p>"},{"location":"classification-project/classification-project/#9-conclusion","title":"9. Conclusion","text":"<ul> <li>The model is excellent at predicting the majority class but weak on minority detection.</li> <li>Future improvements should target raising recall for class 1 \u2014 e.g., by resampling, adjusting thresholds, or rebalancing the loss function.</li> </ul> <p>Findings. A from\u2011scratch NumPy MLP with mini\u2011batch SGD and early stopping reaches strong ranking metrics on the Bank Marketing dataset, outperforming a majority baseline.</p> <p>Limitations. Plain MLPs do not directly address class imbalance and may require threshold tuning; probability calibration is not ensured.</p> <p>Future work. Class\u2011weighting or focal loss, wider architecture/regularization sweeps, feature engineering on contact history/time, and comparison with gradient\u2011boosted trees.</p>"},{"location":"classification-project/classification-project/#submission-github-pages","title":"Submission \u2014 GitHub Pages","text":"<p>This report (Steps 1\u20138, Conclusion, and References) is designed for GitHub Pages (course template compatible). All images live in <code>src/images/</code> and are referenced relatively.</p>"},{"location":"classification-project/classification-project/#academic-integrity-ai-collaboration","title":"Academic Integrity &amp; AI Collaboration","text":"<p>AI assistance was used for code scaffolding, documentation, and figure generation. The authors understand and can explain all parts of the solution; plagiarism policies were respected.</p>"},{"location":"classification-project/classification-project/#references","title":"References","text":"<ol> <li>Moro, S., Cortez, P., &amp; Rita, P. (2014). A Data\u2011Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, 62, 22\u201331. (UCI Bank Marketing)  </li> <li>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back\u2011propagating errors. Nature, 323, 533\u2013536.  </li> <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press (Chs. 6\u20137 for MLPs and optimization).</li> </ol>"},{"location":"data/data_exercise/","title":"Data Task","text":""},{"location":"data/data_exercise/#exercise-1","title":"Exercise 1","text":"<p>The source code for the exercise is available here.</p>"},{"location":"data/data_exercise/#1-distribution-overlap","title":"1. Distribution &amp; Overlap","text":"<ul> <li>Class C0 is a compact cluster on the left, with vertical spread. </li> <li>Class C1 is in the upper-middle region with larger variance, overlapping with C0.</li> <li>Class C2 is in the lower-middle and more compact, mostly separated from other classes.</li> <li>Class C3 is in the far right, tall and narrow, mostly without overlap. </li> </ul> <p>The only overlap is between C0 and C1. C2 and C3 are well separated from other classes. </p>"},{"location":"data/data_exercise/#2-can-a-single-linear-boundary-separate-all-classes","title":"2. Can a single linear boundary separate all classes?","text":"<p>No, a single straight line can separate only in two regions, but we have four classes. Linear models can work ifwe use a multinomial logistic regression (in the image above), separating in four regions, but still struggle to separate C0 and C1. </p>"},{"location":"data/data_exercise/#3-neural-network-decision-boundaries","title":"3. Neural network decision boundaries","text":"<p>A multi-layer perceptron (MLP) can better separate the boundaries between the classes than a linear model (see image above), bending around the classes and capturing the non-linear boundaries. </p>"},{"location":"data/data_exercise/#exercise-2","title":"Exercise 2","text":""},{"location":"data/data_exercise/#1-relationship-between-the-classes","title":"1. Relationship between the classes","text":"<p>Looking at the projection (see image above), we see 2 main clusters:  - Cluster A is in the left, with some variance in both axes.  - Cluster B is in the right, with some variance in both axes.  Both clusters overlap around the origin, so they are not perfectly separable in this projection. </p>"},{"location":"data/data_exercise/#2-linear-separability","title":"2. Linear separability","text":"<p>From the PCA scatter, we can observe that the separation between the classes is not perfectly linear. A boundary between the two clusters may split most points correctly, but the overlap around the origin makes it difficult to perfectly separate the classes.</p> <p>Linear models struggle to separate the 2 classes because they are not able to capture non-linear boundaries. Neural networks with non-linear activation functions can learn non-linear boundaries, separating the classes with higher accuracy by adapting to different shapes of the data. </p>"},{"location":"data/data_exercise/#exercise-3","title":"Exercise 3","text":""},{"location":"data/data_exercise/#dataset","title":"Dataset","text":"<p>Kaggle's \"Spaceship Titanic\" dataset is a binary classification dataset. The task is to predict whether a passenger was transported to an alternate dimension during the collision with the spacetime anomaly. For that, we have the following features: - PassengerId: A unique identifier for each passenger. - HomePlanet: The home planet of the passenger. - CryoSleep: Indicates whether the passenger is in cryosleep for the voyage. - Cabin: The cabin number of the passenger. - Destination: The destination planet of the passenger. - Age: The age of the passenger. - VIP: Indicates whether the passenger is a VIP. - RoomService, FoodCourt, ShoppingMall, Spa, VRDeck: amount the passenger has spent in the ship's services. - Name: The name of the passenger. - Transported: The target variable, indicating whether the passenger was transported to an alternate dimension.</p> <p>Numerical features are:  - Age - RoomService - FoodCourt - ShoppingMall - Spa - VRDeck</p> <p>Categorical features are: - HomePlanet - CryoSleep - Cabin - Destination - VIP</p> <p>The missing values are: - PassengerId: 0 - HomePlanet: 201 - CryoSleep: 217 - Cabin: 199 - Destination: 182 - Age: 179 - VIP: 203 - RoomService: 181 - FoodCourt: 183 - ShoppingMall: 208 - Spa: 183 - VRDeck: 188 - Name: 200 - Transported: 0</p> <p>We can see that the missing values are not random, but are concentrated in some features. We can impute the missing values with the median for numerical features and the most frequent value for categorical features.</p>"},{"location":"data/data_exercise/#preprocessing-the-data","title":"Preprocessing the data","text":"<ul> <li> <p>Missing values: the dataset has missing values in several numerical and categorical features. To prepare the data for a neural network, we can input the missing values:</p> <ul> <li>Numerical features: input the missing values with the median of each column, which is robust against outliers. </li> <li>Categorical features: input the missing values wth the most frequent category (mode). This preserves the overall distribution without introducing unrealistic new labels. </li> </ul> </li> <li> <p>Encoding categorical features: the categorical features cannot be directly interpreted by the neural network, so we can use one-hot encoding to transform each category into binary features. This avoids imposing an order on the categories. </p> </li> <li> <p>Scaling numerical features: since the <code>tanh</code> activation function produces outputs between -1 and 1, we can scale numerical features using a standard scaler, which rescales data so each feature has a mean of 0 and a standard deviation of 1. This ensures that the features are distributed symetrically around 0. Alternatively, we could use a min-max scaler, scaling the features to a range between -1 and 1. </p> </li> </ul>"},{"location":"data/data_exercise/#visualization-and-scaling-effects","title":"Visualization and scaling effects","text":""},{"location":"data/data_exercise/#age-before-and-after-scaling","title":"Age before and after scaling","text":""},{"location":"data/data_exercise/#foodcourt-before-and-after-scaling","title":"FoodCourt before and after scaling","text":"<ul> <li><code>Age</code> originally ranged from 0 to 90. After standardization, it is now centered at 0, but the distribution of the data remains the same. </li> <li>This also happens with <code>FoodCourt</code>. The original data has many zeroes and multiple large values, and after scaling, the distribution remains the same, but the values are normalized relative to other features. </li> </ul> <p>Note: Artificial Intelligence was used in this exercise for code completion and review.</p>"},{"location":"mlp/mlp-exercise/","title":"MLP Task","text":""},{"location":"mlp/mlp-exercise/#mlp-task","title":"MLP Task","text":"<p>The complete source code for this task is available here.</p>"},{"location":"mlp/mlp-exercise/#exercise-1-manual-calculation-of-mlp-steps","title":"Exercise 1 - Manual Calculation of MLP Steps","text":"<p>We consider a MLP with 2 features, 1 layer with 2 neurons, and 1 output neuron, with tanh activations and MSE loss \\(L=\\frac{1}{N}(y-\\hat y)^2\\) (\\(N=1\\)). For the parameter update, use learning rate \\(\\eta=0.1\\).</p> <p>Given</p> \\[ \\mathbf{x}=\\begin{bmatrix}0.5\\\\-0.2\\end{bmatrix},\\quad y=1.0,\\quad \\mathbf{W}^{(1)}=\\begin{bmatrix}0.3&amp;-0.1\\\\0.2&amp;0.4\\end{bmatrix},\\quad \\mathbf{b}^{(1)}=\\begin{bmatrix}0.1\\\\-0.2\\end{bmatrix}, \\] \\[ \\mathbf{W}^{(2)}=\\begin{bmatrix}0.5&amp;-0.3\\end{bmatrix},\\quad b^{(2)}=0.2,\\quad \\text{activation } \\tanh(u),\\quad L=(y-\\hat y)^2. \\]"},{"location":"mlp/mlp-exercise/#1-forward-pass","title":"1. Forward pass","text":"<p>Hidden pre-activation</p> \\[ \\mathbf{z}^{(1)}=\\mathbf{W}^{(1)}\\mathbf{x}+\\mathbf{b}^{(1)} =\\begin{bmatrix}0.3&amp;-0.1\\\\0.2&amp;0.4\\end{bmatrix} \\begin{bmatrix}0.5\\\\-0.2\\end{bmatrix} +\\begin{bmatrix}0.1\\\\-0.2\\end{bmatrix} =\\begin{bmatrix}0.27\\\\-0.18\\end{bmatrix}. \\] <p>Hidden activation</p> \\[ \\mathbf{h}^{(1)}=\\tanh(\\mathbf{z}^{(1)}) =\\begin{bmatrix}\\tanh(0.27)\\\\ \\tanh(-0.18)\\end{bmatrix} \\approx \\begin{bmatrix}0.26362484\\\\-0.17808087\\end{bmatrix}. \\] <p>Output pre-activation</p> \\[ u^{(2)}=\\mathbf{W}^{(2)}\\mathbf{h}^{(1)}+b^{(2)} =0.5\\cdot 0.26362484+(-0.3)\\cdot(-0.17808087)+0.2 \\approx 0.3852366782. \\] <p>Output activation and loss</p> \\[ \\hat y=\\tanh(u^{(2)})\\approx \\tanh(0.3852366782)\\approx 0.3672465626, \\] \\[ L=(y-\\hat y)^2=(1-0.3672465626)^2\\approx 0.4003769125. \\]"},{"location":"mlp/mlp-exercise/#2-backward-pass","title":"2. Backward pass","text":"<p>Useful derivative: $$ \\frac{d}{du}\\tanh(u)=1-\\tanh^2(u). $$</p> <p>From loss to output:</p> \\[ \\frac{\\partial L}{\\partial \\hat y}=2(\\hat y-1)\\approx -1.2655068747, \\] \\[ \\frac{\\partial \\hat y}{\\partial u^{(2)}}=1-\\tanh^2(u^{(2)})=1-\\hat y^2\\approx 0.8651299622, \\] \\[ \\frac{\\partial L}{\\partial u^{(2)}}=\\frac{\\partial L}{\\partial \\hat y}\\cdot \\frac{\\partial \\hat y}{\\partial u^{(2)}} \\approx (-1.2655068747)\\cdot 0.8651299622\\approx -1.0948279147. \\] <p>Output layer gradients:</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}}=\\frac{\\partial L}{\\partial u^{(2)}}\\,(\\mathbf{h}^{(1)})^\\top \\approx \\begin{bmatrix} -1.0948279147\\cdot 0.26362484 &amp; -1.0948279147\\cdot (-0.17808087) \\end{bmatrix} \\approx \\begin{bmatrix}-0.28862383 &amp; 0.19496791\\end{bmatrix}, \\] \\[ \\frac{\\partial L}{\\partial b^{(2)}}=\\frac{\\partial L}{\\partial u^{(2)}}\\approx -1.0948279147. \\] <p>Backpropagated to hidden activation:</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}}= \\left(\\frac{\\partial L}{\\partial u^{(2)}}\\right)(\\mathbf{W}^{(2)})^\\top \\approx -1.0948279147\\cdot \\begin{bmatrix}0.5\\\\-0.3\\end{bmatrix} \\approx \\begin{bmatrix}-0.54741396\\\\ 0.32844837\\end{bmatrix}. \\] <p>Hidden activation derivative:</p> \\[ 1-\\tanh^2(\\mathbf{z}^{(1)})= \\begin{bmatrix} 1-\\tanh^2(0.27)\\\\[2pt] 1-\\tanh^2(-0.18) \\end{bmatrix} \\approx \\begin{bmatrix} 0.93050195\\\\ 0.96828720 \\end{bmatrix}. \\] <p>Hidden pre-activation gradient:</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}}= \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}}\\odot \\left(1-\\tanh^2(\\mathbf{z}^{(1)})\\right) \\approx \\begin{bmatrix}-0.54741396\\\\ 0.32844837\\end{bmatrix}\\odot \\begin{bmatrix}0.93050195\\\\ 0.96828720\\end{bmatrix} \\approx \\begin{bmatrix}-0.50936975\\\\ 0.31803236\\end{bmatrix}. \\] <p>Hidden layer parameter gradients:</p> \\[ \\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}}= \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}}\\,\\mathbf{x}^\\top = \\begin{bmatrix}-0.50936975\\\\ 0.31803236\\end{bmatrix} \\begin{bmatrix}0.5 &amp; -0.2\\end{bmatrix} \\approx \\begin{bmatrix} -0.25468488 &amp; 0.10187395\\\\ \\ \\ 0.15901618 &amp; -0.06360647 \\end{bmatrix}, \\] \\[ \\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}}=\\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} \\approx \\begin{bmatrix}-0.50936975\\\\ 0.31803236\\end{bmatrix}. \\]"},{"location":"mlp/mlp-exercise/#3-parameter-update-gradient-descent","title":"3. Parameter update (gradient descent)","text":"<p>Output layer:</p> \\[ \\mathbf{W}^{(2)}\\leftarrow \\mathbf{W}^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\begin{bmatrix}0.5&amp;-0.3\\end{bmatrix} -0.1\\begin{bmatrix}-0.28862383&amp;0.19496791\\end{bmatrix} = \\begin{bmatrix}0.52886238&amp;-0.31949679\\end{bmatrix}, \\] \\[ b^{(2)}\\leftarrow b^{(2)}-\\eta\\,\\frac{\\partial L}{\\partial b^{(2)}} =0.2-0.1(-1.0948279147)\\approx 0.3094827915. \\] <p>Hidden layer: $$ \\mathbf{W}^{(1)}\\leftarrow \\mathbf{W}^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\begin{bmatrix}0.3&amp;-0.1\\ 0.2&amp;0.4\\end{bmatrix} -0.1 \\begin{bmatrix} -0.25468488 &amp; 0.10187395\\   0.15901618 &amp; -0.06360647 \\end{bmatrix} \\approx \\begin{bmatrix} 0.32546849 &amp; -0.11018740\\ 0.18409838 &amp;   0.40636065 \\end{bmatrix}, $$</p> \\[ \\mathbf{b}^{(1)}\\leftarrow \\mathbf{b}^{(1)}-\\eta\\,\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\begin{bmatrix}0.1\\\\-0.2\\end{bmatrix} -0.1\\begin{bmatrix}-0.50936975\\\\ 0.31803236\\end{bmatrix} \\approx \\begin{bmatrix}0.15093698\\\\-0.23180324\\end{bmatrix}. \\]"},{"location":"mlp/mlp-exercise/#4-final-parameters","title":"4. Final parameters","text":"<ul> <li> <p>\\(\\mathbf{W}^{(2)} \\approx [\\,0.52886238,\\ -0.31949679\\,]\\)</p> </li> <li> <p>\\(b^{(2)} \\approx 0.3094827915\\)</p> </li> <li> <p>\\(\\displaystyle \\mathbf{W}^{(1)} \\approx \\begin{bmatrix} 0.32546849 &amp; -0.11018740\\\\ 0.18409838 &amp; \\ \\ 0.40636065 \\end{bmatrix}\\)</p> </li> <li> <p>\\(\\displaystyle \\mathbf{b}^{(1)} \\approx \\begin{bmatrix}0.15093698\\\\-0.23180324\\end{bmatrix}\\)</p> </li> </ul>"},{"location":"mlp/mlp-exercise/#exercise-2-binary-classification-and-mlp","title":"Exercise 2 \u2014 Binary Classification and MLP","text":"<p>We need to build a 2D binary dataset where class 0 has 1 cluster and class 1 has 2 clusters. We will use <code>make_classification</code> and compose the subsets to create this. After that, we will train a NumPy only MLP with 1 hidden tanh layer with 16 neurons, and 1 output neuron (sigmoid + BCE loss). </p>"},{"location":"mlp/mlp-exercise/#1-data-generation","title":"1. Data generation","text":"<p>Here, we generate two datasets and keep only the slice we need from each, then stack and shuffle the data.</p> <pre><code>def _take_class(X, y, cls, n_needed):\n    Xc = X[y == cls]\n    if Xc.shape[0] &lt; n_needed:\n        raise ValueError(\"Increase n_samples in generator\")\n    return Xc[:n_needed]\n\ndef binary_1v2_clusters(n_total=1000, random_state=42, class_sep=1.5):\n    n0 = n1 = n_total // 2\n\n    Xa, ya = make_classification(\n        n_samples=3*n0, n_features=2, n_informative=2, n_redundant=0,\n        n_classes=2, n_clusters_per_class=1, class_sep=class_sep,\n        flip_y=0.0, random_state=random_state\n    )\n    X0 = _take_class(Xa, ya, cls=0, n_needed=n0)\n\n    Xb, yb = make_classification(\n        n_samples=3*n1, n_features=2, n_informative=2, n_redundant=0,\n        n_classes=2, n_clusters_per_class=2, class_sep=class_sep,\n        flip_y=0.0, random_state=random_state + 1\n    )\n    X1 = _take_class(Xb, yb, cls=1, n_needed=n1)\n\n    X = np.vstack([X0, X1])\n    y = np.hstack([np.zeros(n0, dtype=int), np.ones(n1, dtype=int)])\n\n    idx = np.random.default_rng(random_state + 2).permutation(n_total)\n    return X[idx], y[idx]\n\nrng = set_all_seeds(42)\nX, y = binary_1v2_clusters(n_total=1000, random_state=42, class_sep=1.5)\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, seed=7)\nytr_b = ytr.reshape(-1, 1)\n</code></pre> <p>We get this data distribution plot:</p> <p></p>"},{"location":"mlp/mlp-exercise/#2-mlp-implementation","title":"2. MLP implementation","text":"<p>We will implement a MLP with 1 hidden tanh layer with 16 neurons, and 1 output neuron (sigmoid + BCE loss). First, we implement the layer class:</p> <pre><code>class Dense:\n    def __init__(self, in_dim, out_dim, activation=None, seed=None):\n        self.in_dim, self.out_dim = in_dim, out_dim\n        rng = np.random.default_rng(seed)\n        std = np.sqrt(1.0 / in_dim)\n        self.W = rng.normal(scale=std, size=(in_dim, out_dim))\n        self.b = np.zeros(out_dim)\n        self.activation = activation\n        self._cache = {}\n\n    def _act(self, z):\n        if self.activation is None:         \n            return z\n        if self.activation == \"tanh\":       \n            return tanh(z)\n        if self.activation == \"sigmoid\":    \n            return sigmoid(z)\n        raise ValueError(f\"unknown activation {self.activation}\")\n\n    def _dact(self, z):\n        if self.activation is None:         \n            return np.ones_like(z)\n        if self.activation == \"tanh\":       \n            return dtanh(z)\n        if self.activation == \"sigmoid\":\n            s = sigmoid(z)\n            return s*(1.0 - s)\n        raise ValueError(f\"unknown activation {self.activation}\")\n\n    def forward(self, X):\n        Z = X @ self.W + self.b\n        A = self._act(Z)\n        self._cache[\"X\"], self._cache[\"Z\"], self._cache[\"A\"] = X, Z, A\n        return A\n\n    def backward(self, dA, lr):\n        X, Z, A = self._cache[\"X\"], self._cache[\"Z\"], self._cache[\"A\"]\n        dZ = dA * self._dact(Z)\n        dW = X.T @ dZ / X.shape[0]\n        db = dZ.mean(axis=0)\n        dX = dZ @ self.W.T\n        self.W -= lr * dW\n        self.b -= lr * db\n        return dX\n</code></pre> <p>Then, we implement the MLP class, that uses the layer class:</p> <pre><code>class MLP:\n    def __init__(self, layer_dims, hidden_activation=\"tanh\", seed=None):\n        self.layers = []\n        rng = np.random.default_rng(seed)\n\n        for i in range(len(layer_dims) - 1):\n            act = hidden_activation if i &lt; len(layer_dims) - 2 else \"sigmoid\"\n            self.layers.append(\n                Dense(layer_dims[i], layer_dims[i+1], activation=act,\n                      seed=int(rng.integers(1 &lt;&lt; 30)))\n            )\n\n    def forward(self, X):\n        A = X\n        for lyr in self.layers:\n            A = lyr.forward(A)\n        return A \n\n    def _bce_and_grad(self, A, y):\n        eps = 1e-12\n        A = np.clip(A, eps, 1.0 - eps)\n        loss = -(y*np.log(A) + (1-y)*np.log(1-A)).mean()\n        dZ = (A - y)\n        return float(loss), dZ\n\n    def fit(self, X, y, epochs=200, lr=1e-1, batch_size=64, seed=None, verbose=False):\n        rng = np.random.default_rng(seed)\n        n = X.shape[0]\n        hist = {\"loss\": []}\n\n        for ep in range(epochs):\n            idx = rng.permutation(n)\n            Xs, ys = X[idx], y[idx]\n            ep_losses = []\n\n            for i in range(0, n, batch_size):\n                xb = Xs[i:i+batch_size]\n                yb = ys[i:i+batch_size]\n                A = self.forward(xb)\n                loss, dZlast = self._bce_and_grad(A, yb)\n                ep_losses.append(loss)\n\n                dA = dZlast\n                for lyr in reversed(self.layers):\n                    dA = lyr.backward(dA, lr)\n\n            hist[\"loss\"].append(np.mean(ep_losses))\n            if verbose and (ep+1) % 20 == 0:\n                print(f\"epoch {ep+1}: loss={hist['loss'][-1]:.4f}\")\n        return hist\n\n    def predict(self, X):\n        A = self.forward(X).ravel()\n        return (A &gt;= 0.5).astype(int)\n</code></pre>"},{"location":"mlp/mlp-exercise/#3-training-and-results","title":"3. Training and results","text":"<p>Finally, we train the MLP in the generated dataset with the following code:</p> <pre><code>mlp = MLP(layer_dims=[2, 16, 1], hidden_activation=\"tanh\", seed=7)\nhist = mlp.fit(Xtr, ytr_b, epochs=200, lr=1e-1, batch_size=64, seed=0, verbose=True)\n</code></pre> <p>We get the following training loss plot:</p> <p></p> <p>In this test, we got a test accuracy of 0.9400.</p> <p>The confusion matrix is:</p> \\[ \\begin{bmatrix} 107 &amp; 4 \\\\ 8 &amp; 81 \\end{bmatrix} \\]"},{"location":"mlp/mlp-exercise/#exercise-3-multiclass-classification-with-mlp","title":"Exercise 3 \u2014 Multiclass classification with MLP","text":"<p>We need to build a multiclass dataset with 3 classes, 4 features, and 2, 3 and 4 clusters per class, respectively. We will use <code>make_classification</code> and compose the subsets to create this. After that, we will make slight changes to the Dense and MLP classes previously implemented, now supporting softmax and cross-entropy loss for multiclass classification. Finally, we will train the MLP.</p>"},{"location":"mlp/mlp-exercise/#1-data-generation_1","title":"1. Data generation","text":"<p>Here, we generate three datasets and keep only the slice we need from each, then stack and shuffle the data. Also, we perform a PCA to reduce the dimensionality to 2D for visualization.</p> <pre><code>def _take_class(X, y, cls, n_needed):\n    Xc = X[y == cls]\n    if Xc.shape[0] &lt; n_needed:\n        raise ValueError(\"Increase n_samples; not enough points for requested slice.\")\n    return Xc[:n_needed]\n\ndef multiclass_2_3_4_clusters(n_total=1500, random_state=42, class_sep=1.5):\n    n_per = n_total // 3\n\n    Xa, ya = make_classification(\n        n_samples=3*n_per, n_features=4, n_informative=4, n_redundant=0,\n        n_classes=3, n_clusters_per_class=2, class_sep=class_sep,\n        flip_y=0.0, random_state=random_state\n    )\n    X0 = _take_class(Xa, ya, cls=0, n_needed=n_per)\n\n    Xb, yb = make_classification(\n        n_samples=3*n_per, n_features=4, n_informative=4, n_redundant=0,\n        n_classes=3, n_clusters_per_class=3, class_sep=class_sep,\n        flip_y=0.0, random_state=random_state+1\n    )\n    X1 = _take_class(Xb, yb, cls=1, n_needed=n_per)\n\n    Xc, yc = make_classification(\n        n_samples=3*n_per, n_features=4, n_informative=4, n_redundant=0,\n        n_classes=3, n_clusters_per_class=4, class_sep=class_sep,\n        flip_y=0.0, random_state=random_state+2\n    )\n    X2 = _take_class(Xc, yc, cls=2, n_needed=n_per)\n\n    X = np.vstack([X0, X1, X2])\n    y = np.hstack([\n        np.zeros(n_per, dtype=int),\n        np.ones(n_per, dtype=int),\n        2*np.ones(n_per, dtype=int),\n    ])\n\n    idx = np.random.default_rng(random_state+3).permutation(n_total)\n    return X[idx], y[idx]\n\nX, y = multiclass_2_3_4_clusters(n_total=1500, random_state=42, class_sep=1.6)\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, seed=7)\nytr_oh = one_hot(ytr, 3)\n\npca = PCA(n_components=2, random_state=7).fit(X)\nX2 = pca.transform(X)\n</code></pre> <p>Here is the data distribution plot after the PCA:</p> <p></p>"},{"location":"mlp/mlp-exercise/#2-mlp-changes","title":"2. MLP changes","text":"<p>We need to make slight changes to the Dense and MLP classes previously implemented, now supporting softmax and cross-entropy loss for multiclass classification. Here is the new Dense class, now supporting all the mentioned activations:</p> <pre><code>def tanh(z):      \n    return np.tanh(z)\ndef dtanh(z):     \n    a = np.tanh(z)\n    return 1.0 - a*a\ndef sigmoid(z):   \n    return 1.0 / (1.0 + np.exp(-z))\ndef softmax(z):\n    zmax = z.max(axis=1, keepdims=True)\n    e = np.exp(z - zmax)\n    return e / e.sum(axis=1, keepdims=True)\n\nclass Dense:\n    def __init__(self, in_dim, out_dim, activation=None, seed=None):\n        self.in_dim, self.out_dim = in_dim, out_dim\n        rng = np.random.default_rng(seed)\n        std = np.sqrt(1.0 / in_dim)\n        self.W = rng.normal(scale=std, size=(in_dim, out_dim))\n        self.b = np.zeros(out_dim)\n        self.activation = activation\n        self._cache = {}\n\n    def _act(self, z):\n        if self.activation is None:         \n            return z\n        if self.activation == \"tanh\":       \n            return tanh(z)\n        if self.activation == \"sigmoid\":    \n            return sigmoid(z)\n        if self.activation == \"softmax\":\n            return softmax(z)\n        raise ValueError(f\"unknown activation {self.activation}\")\n\n    def _dact(self, z):\n        if self.activation is None:         \n            return np.ones_like(z)\n        if self.activation == \"tanh\":       \n            return dtanh(z)\n        if self.activation == \"sigmoid\":\n            s = sigmoid(z)\n            return s*(1.0 - s)\n        if self.activation == \"softmax\":\n            return None\n        raise ValueError(f\"unknown activation {self.activation}\")\n\n    def forward(self, X):\n        Z = X @ self.W + self.b\n        A = self._act(Z)\n        self._cache[\"X\"], self._cache[\"Z\"], self._cache[\"A\"] = X, Z, A\n        return A\n\n    def backward(self, dA, lr):\n        X, Z, A = self._cache[\"X\"], self._cache[\"Z\"], self._cache[\"A\"]\n        if self.activation == \"softmax\":\n            dZ = dA\n        else:\n            dZ = dA * self._dact(Z)\n        dW = X.T @ dZ / X.shape[0]\n        db = dZ.mean(axis=0)\n        dX = dZ @ self.W.T\n        self.W -= lr * dW\n        self.b -= lr * db\n        return dX\n</code></pre> <p>And here is the new MLP class, now supporting softmax and cross-entropy loss for multiclass classification:</p> <pre><code>class MLP:\n    def __init__(self, layer_dims, hidden_activation=\"tanh\", multiclass=True, seed=None):\n        self.layers = []\n        self.multiclass = multiclass\n        rng = np.random.default_rng(seed)\n\n        for i in range(len(layer_dims) - 1):\n            act = hidden_activation if i &lt; len(layer_dims) - 2 else (\"softmax\" if multiclass else None)\n            self.layers.append(\n                Dense(layer_dims[i], layer_dims[i+1], activation=act,\n                      seed=int(rng.integers(1 &lt;&lt; 30)))\n            )\n\n    def forward(self, X):\n        A = X\n        for lyr in self.layers:\n            A = lyr.forward(A)\n        return A\n\n    def _softmax_ce_and_grad(self, P, Y_onehot):\n        eps = 1e-12\n        P = np.clip(P, eps, 1.0)\n        loss = -(Y_onehot * np.log(P)).sum(axis=1).mean()\n        dZ = (P - Y_onehot)\n        return float(loss), dZ\n\n    def fit(self, X, y_onehot, epochs=300, lr=1e-1, batch_size=64, seed=None, verbose=False):\n        rng = np.random.default_rng(seed)\n        n = X.shape[0]\n        hist = {\"loss\": []}\n\n        for ep in range(epochs):\n            idx = rng.permutation(n)\n            Xs, ys = X[idx], y_onehot[idx]\n            ep_losses = []\n\n            for i in range(0, n, batch_size):\n                xb = Xs[i:i+batch_size]\n                yb = ys[i:i+batch_size]\n\n                P = self.forward(xb)        \n                loss, dZlast = self._softmax_ce_and_grad(P, yb)\n                ep_losses.append(loss)\n\n                dA = dZlast\n                for lyr in reversed(self.layers):\n                    dA = lyr.backward(dA, lr)\n\n            hist[\"loss\"].append(np.mean(ep_losses))\n            if verbose and (ep+1) % 25 == 0:\n                print(f\"epoch {ep+1:3d}: loss={hist['loss'][-1]:.4f}\")\n        return hist\n\n    def predict(self, X):\n        P = self.forward(X)\n        return np.argmax(P, axis=1)\n</code></pre>"},{"location":"mlp/mlp-exercise/#3-training-and-results_1","title":"3. Training and results","text":"<p>Finally, we train the MLP in the generated dataset with the following code:</p> <pre><code>mlp3 = MLP(layer_dims=[4, 32, 3], hidden_activation=\"tanh\", multiclass=True, seed=7)\nhist3 = mlp3.fit(Xtr, one_hot(ytr, 3), epochs=500, lr=1e-1, batch_size=64, seed=0, verbose=True)\n</code></pre> <p>We get the following training loss plot:</p> <p></p> <p>In this test with 500 epochs, we got a test accuracy of 0.9600.</p> <p>The confusion matrix is:</p> <p></p>"},{"location":"mlp/mlp-exercise/#exercise-4-deeper-mlp-on-multiclass-dataset","title":"Exercise 4 \u2014 Deeper MLP on multiclass dataset","text":"<p>We will now use a generated multiclass dataset, with the same code and parameters as the previous exercise, but now with a deeper MLP. For this exercise, we will use a MLP with 3 hidden layers, with 64, 64 and 32 neurons, respectively, and tanh activations. We will skip the data preparation step as it is the same as the previous exercise. Here is the 2D PCA plot of the train data:</p> <p></p> <p>No changes are needed to the MLP class, as it is already supporting softmax and cross-entropy loss for multiclass classification, and multiple hidden layers with selectable number of neurons.</p> <p>We train the MLP with the following code:</p> <pre><code>mlp4 = MLP(layer_dims=[4, 64, 64, 32, 3], hidden_activation=\"tanh\", multiclass=True, seed=11)\nhist4 = mlp4.fit(Xtr, ytr_oh, epochs=500, lr=1e-1, batch_size=64, seed=0, verbose=True)\n</code></pre> <p>We get the following training loss plot:</p> <p></p> <p>In this test with 500 epochs, we got a test accuracy of 0.9633.</p> <p>The confusion matrix is:</p> <p></p> <p>The deeper MLP with two hidden layers achieved slightly better accuracy compared to the single hidden layer MLP from Exercise 3. However, the improvement was modest, as the one hidden layer architecture already performed very well. This shows that for relatively simple synthetic tasks, increasing depth may not yield significant gains, though it can provide small improvements in stability and performance.</p> <p>Note: Artificial intelligence was used in this exercise for code completion and review, for translating math formulas to Latex, and for text review.</p>"},{"location":"perceptron/perceptron-exercise/","title":"Perceptron Task","text":""},{"location":"perceptron/perceptron-exercise/#perceptron-task","title":"Perceptron Task","text":"<p>The complete source code for this task is available here.</p>"},{"location":"perceptron/perceptron-exercise/#exercise-1","title":"Exercise 1","text":""},{"location":"perceptron/perceptron-exercise/#data-generation","title":"Data Generation","text":"<p>To begin with, a sample of 2D points from two multivariate normals (1000 samples per class) is generated:</p> <ul> <li> <p>Class 0: mean = <code>[1.5, 1.5]</code>, covariance = <code>[[0.5, 0], [0, 0.5]]</code></p> </li> <li> <p>Class 1: mean = <code>[5.0, 5.0]</code>, covariance = <code>[[0.5, 0], [0, 0.5]]</code></p> </li> </ul> <p>Code Snippet:</p> <pre><code>n_per_class = 1000\n\nmean0 = np.array([1.5, 1.5])\ncov0  = np.array([[0.5, 0.0], [0.0, 0.5]])\n\nmean1 = np.array([5.0, 5.0])\ncov1  = np.array([[0.5, 0.0], [0.0, 0.5]])\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\nX  = np.vstack([X0, X1])\ny01 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\n\nplot_data(X, y01, title=\"Exercise 1: Data (mostly separable)\", save_name=\"exercise1-data\")\n</code></pre> <p>This creates two clusters that are mostly linearly separable.</p> <p></p>"},{"location":"perceptron/perceptron-exercise/#single-layer-perceptron","title":"Single Layer Perceptron","text":"<p>After that, a single-layer perceptron is implemented from scratch using NumPy for linear algebra. The labels are mapped from <code>{0,1}</code> to <code>{-1,+1}</code>.</p> <p>Code Snippet:</p> <pre><code>class Perceptron:\n    \"\"\"\n    Single layer perceptron implementation\n    Update rule:\n      w &lt;- w + eta * y * x\n      b &lt;- b + eta * y\n    Runs updates over epochs until no mistakes or max_epochs\n    \"\"\"\n    def __init__(self, eta=0.01, max_epochs=100, shuffle=True, seed=None):\n        self.eta = float(eta)\n        self.max_epochs = int(max_epochs)\n        self.shuffle = bool(shuffle)\n        self.seed = seed\n        self.w = None\n        self.b = None\n        self.history = {\"epoch_acc\": []}\n\n    def _sign(self, z):\n        # classify &gt;=0 as +1 else -1\n        return np.where(z &gt;= 0, 1, -1)\n\n    def fit(self, X, y_pm1):\n        n, d = X.shape\n        rng_local = np.random.default_rng(self.seed)\n\n        # init small weights\n        self.w = rng_local.normal(scale=0.01, size=d)\n        self.b = 0.0\n\n        for _ in range(self.max_epochs):\n            if self.shuffle:\n                idx = rng_local.permutation(n)\n            else:\n                idx = np.arange(n)\n\n            updates = 0\n            for i in idx:\n                xi = X[i]\n                yi = y_pm1[i]\n                pred = self._sign(np.dot(self.w, xi) + self.b)\n                if pred != yi:\n                    self.w = self.w + self.eta * yi * xi\n                    self.b = self.b + self.eta * yi\n                    updates += 1\n\n            y_pred = self.predict(X)\n            acc = accuracy(y_pm1, y_pred)\n            self.history[\"epoch_acc\"].append(acc)\n\n            if updates == 0:\n                break\n\n        return self\n\n    def predict(self, X):\n        z = X @ self.w + self.b\n        return self._sign(z)\n</code></pre>"},{"location":"perceptron/perceptron-exercise/#training-results","title":"Training &amp; Results","text":"<p>Finally, the perceptron is trained for up to 100 epochs (early-stopping if no updates in an epoch).</p> <p>Code Snippet:</p> <pre><code>perc_e1 = Perceptron(eta=0.01, max_epochs=100, shuffle=True, seed=7)\nperc_e1.fit(X, y_pm1)\n\ny_pred_e1 = perc_e1.predict(X)\nacc_e1 = accuracy(y_pm1, y_pred_e1)\n</code></pre> <p>With that, the final results obtained are:</p> <ul> <li> <p>Final Accuracy: <code>1.0000</code></p> </li> <li> <p>Final Weights: <code>[0.01332216 0.01551652]</code></p> </li> <li> <p>Final Bias: <code>-0.09999999999999999</code></p> </li> </ul>"},{"location":"perceptron/perceptron-exercise/#decision-boundary","title":"Decision Boundary","text":"<p>The decision boundary was plotted as follows:</p> <p></p> <p>Only one epoch was needed to achieve 100% accuracy, as the data is linearly separable. All points were correctly classified.</p> <p></p>"},{"location":"perceptron/perceptron-exercise/#discussion","title":"Discussion","text":"<p>Because the two classes are linearly separable, with means far apart and low variance, the perceptron converges in very few passes with near perfect accuracy. This illustrates the Perceptron's theory: if the data is linearly separable, the aglorithm will find a separating hyperplane after a finite number of updates. In this case, the data is perfectly separable, so the perceptron converges in one epoch.</p>"},{"location":"perceptron/perceptron-exercise/#exercise-2","title":"Exercise 2","text":""},{"location":"perceptron/perceptron-exercise/#data-generation_1","title":"Data Generation","text":"<p>First, 2D points were sampled (1000 samples per class):</p> <ul> <li> <p>Class 0: mean = <code>[3.0, 3.0]</code>, covariance = <code>[[1.5, 0], [0, 1.5]]</code></p> </li> <li> <p>Class 1: mean = <code>[4.0, 4.0]</code>, covariance = <code>[[1.5, 0], [0, 1.5]]</code></p> </li> </ul> <p>Code Snippet:</p> <pre><code>n_per_class = 1000\n\nmean0 = np.array([3.0, 3.0])\ncov0  = np.array([[1.5, 0.0], [0.0, 1.5]])\n\nmean1 = np.array([4.0, 4.0])\ncov1  = np.array([[1.5, 0.0], [0.0, 1.5]])\n\nX0 = rng.multivariate_normal(mean0, cov0, size=n_per_class)\nX1 = rng.multivariate_normal(mean1, cov1, size=n_per_class)\nX2 = np.vstack([X0, X1])\ny01_2 = np.hstack([np.zeros(n_per_class, dtype=int), np.ones(n_per_class, dtype=int)])\n\nplot_data(X2, y01_2, title=\"Exercise 2: Data (overlapping)\", save_name=\"exercise2-data\")\n</code></pre> <p>This creates 2 clusters that are partially overlapping, so the data is not linearly separable.</p> <p></p>"},{"location":"perceptron/perceptron-exercise/#perceptron","title":"Perceptron","text":"<p>In order to train, we reuse the same class and training loop from Exercise 1.</p>"},{"location":"perceptron/perceptron-exercise/#training-results_1","title":"Training &amp; Results","text":"<p>The perceptron is trained for up to 100 epochs (early-stopping if no updates in an epoch), using the same training loop as Exercise 1.</p> <p>With that, the final results obtained were:</p> <ul> <li> <p>Final Accuracy: <code>0.6375</code></p> </li> <li> <p>Final Weights: <code>[0.09423536 0.05907569]</code></p> </li> <li> <p>Final Bias: <code>-0.41000000000000014</code></p> </li> </ul> <p>Decision Boundary</p> <p></p> <p>Accuracy over epochs</p> <p></p> <p>Misclassified points</p> <p></p>"},{"location":"perceptron/perceptron-exercise/#discussion_1","title":"Discussion","text":"<p>Because the classes overlap, the perceptron cannot perfectly separate them, breaking the assumption of linear separability. As expected, the convergence did not occur within 100 epochs, continuing to make updates in each epoch. This happens because some samples are impossible to classify correctly with a single linear separator, so fixing one sample may cause another to be misclassified.</p> <p>The accuracy of the perceptron ranged from 0.50 to 0.70 over the epochs, indicating that the perceptron was not able to converge to a perfect classifier. The comparison with Exercise 1 shows both the weakness and the strength of the perceptron. With perfectly separable data, it converges in one epoch (or few epochs), but with overlapping data it becomes unstable and unable to converge. </p> <p>Note: Artificial Intelligence was used in this exercise for code completion and review, and for text revision.</p>"},{"location":"projeto/main/","title":"Main","text":"<p>Aqui vai toda a documenta\u00e7\u00e3o do projeto, incluindo o que j\u00e1 foi feito e o que falta fazer.</p>"},{"location":"regression-project/regression-project/","title":"Regression Project","text":""},{"location":"regression-project/regression-project/#regression-project","title":"Regression Project","text":"<p>The complete source code for this project is available here.</p>"},{"location":"regression-project/regression-project/#1-dataset-selecion","title":"1. Dataset Selecion","text":"<p>Name: Wine Quality Dataset</p> <p>Source: UCI Machine Learning Repository available here</p> <p>Size: 6497 rows and 11 features</p> <p>Task: Regression. The target feature is a score between 1 and 10 for the quality of each wine, and is treated as a regression task in the UCI repository. </p> <p>Why this dataset? </p> <p>To begin with, all members of the group love wine. Moreover, the dataset is well-known, tabular, and does not include time-series features. It contains a mix of continuous features, and is divided into red wine and white wine, so we created a new categorical feature <code>type</code> to reflect that information and concatenated the two datasets. It relates to a real-world problem that the group is passionate about, which is predicting the quality of the wine based on mensurable lab features. The dataset also has no missing values. </p>"},{"location":"regression-project/regression-project/#2-dataset-explanation","title":"2. Dataset Explanation","text":"<p>Description: This dataset represents physicochemical laboratory tests performed on different wine samples. Each row corresponds to a wine sample, described by 11 numerical features such as acidity, sugar and pH, one categorical feature (created by us) indicating whether the wine is red or white, and one target feature representing the quality score assigned by human sensory evaluators. </p> <p>Features:</p> <ul> <li> <p>Fixed Acidity: The amount of non-volatile acids in the wine, which affects its taste and stability.</p> </li> <li> <p>Volatile Acidity: The amount of acetic acid in the wine, which can lead to an unpleasant vinegar taste if too high.</p> </li> <li> <p>Citric Acid: A natural acid found in wine that can add freshness and flavor.</p> </li> <li> <p>Residual Sugar: The amount of sugar remaining in the wine after fermentation, which can influence sweetness.</p> </li> <li> <p>Chlorides: The amount of salt in the wine, which can affect its taste and preservation.</p> </li> <li> <p>Free Sulfur Dioxide: The amount of free SO2 in the wine, which acts as a preservative and antioxidant.</p> </li> <li> <p>Total Sulfur Dioxide: The total amount of SO2 in the wine, including both free and bound forms.</p> </li> <li> <p>Density: The density of the wine, which can indicate alcohol content and sugar levels.</p> </li> <li> <p>pH: The acidity level of the wine, which can affect its taste and stability.</p> </li> <li> <p>Sulphates: The amount of sulphates in the wine, which can contribute to its flavor and preservation.</p> </li> <li> <p>Alcohol: The alcohol content of the wine, which can influence its body and taste.</p> </li> <li> <p>Type: A categorical feature created by us indicating whether the wine is red or white.</p> </li> </ul> <p>Target Feature:</p> <ul> <li>Quality: A score between 1 and 10 assigned by human sensory evaluators, representing the overall quality of the wine. Though integer valued, it is modeled as continuous, as differences between values represent intensity variation (ordinal regression setup, but treated here as standard numeric regression).</li> </ul> <p>Dataset Numerical Summary:</p> <p></p> <p>The summary of numerical features show that most features have varying scales and distributions, with very low values like density and chlorides, and very high values like total sulfur dioxide. Moreover, some features like residual sugar, total sulfur dioxide, and free sulfur dioxide exhibit right-skewed distributions, indicating that a small number of samples have significantly higher values compared to the rest. This suggests that feature scaling and potential transformations may be necessary during preprocessing to ensure effective model training.</p> <p>The dataset contains 4898 samples of white wine and 1599 samples of red wine, indicating a class imbalance that may need to be addressed. </p> <p>Visualizations:</p> <p>The following visualizations provide insights into the dataset's characteristics:</p> <ul> <li>Histogram of Quality Scores:</li> </ul> <p></p> <p>The histogram shows that most wines have quality scores between 5 and 7, with fewer samples at the extreme ends of the scale. This indicates a moderate distribution of quality ratings.</p> <ul> <li> <p>Type Distribution:</p> <p></p> <p>The bar chart illustrates the class imbalance between red and white wines, with white wines being more prevalent in the dataset.</p> </li> <li> <p>Correlation Heatmap:</p> <p></p> <p>The heatmap reveals the correlations between different features and the target variable (quality). Notably, alcohol content shows a positive correlation with quality, while volatile acidity exhibits a negative correlation, but no strong linear relationships are evident.</p> </li> <li> <p>Boxplots of Continuous Features:</p> <p></p> <p>The boxplots highlight the distribution and potential outliers in each continuous feature, indicating variability in the data. Most continuous features show a large number of outliers, which will be addressed during preprocessing.</p> </li> <li> <p>Relationships Between Top Features and Quality:</p> <p></p> <p>The pairplot illustrates the relationships between the top correlated features and quality, revealing potential non-linear patterns that may be important for modeling, but nothing really stands out strongly.</p> </li> </ul>"},{"location":"regression-project/regression-project/#3-data-cleaning-and-normalization","title":"3. Data Cleaning and Normalization","text":""},{"location":"regression-project/regression-project/#overview","title":"Overview","text":"<p>The preprocessing stage focused on ensuring data consistency, proper scaling, and avoiding information leakage. Because all original features were numerical except for the created <code>type</code> feature, we applied different strategies for numerical and categorical features, with standardization and one-hot encoding applied only after fitting on the training data. Additionally, outliers in numerical features were addressed using a custom transformer that clips values based on the 1st and 99th percentiles from the training set.</p>"},{"location":"regression-project/regression-project/#data-cleaning","title":"Data Cleaning","text":"<p>The loaded dataset does not include any missing values, so no imputation was necessary. Column names were standardized to lowercase snake_case for consistency. Here is how we handled the loading, concatenation and feature creation:</p> <pre><code>red_wine = pd.read_csv('data/winequality-red.csv', sep=';')\nred_wine['type'] = 'red'\nwhite_wine = pd.read_csv('data/winequality-white.csv', sep=';')\nwhite_wine['type'] = 'white'\n\ndata = pd.concat([red_wine, white_wine], ignore_index=True)\n</code></pre>"},{"location":"regression-project/regression-project/#data-splitting","title":"Data Splitting","text":"<p>To preserve the distribution of the target variable, the dataset was divided using a stratified sampling, based on quantile bins of the quality scores. This ensures that low quality and high quality wines are proportionally represented in each subset. The data was split into training (70%), validation (15%), and test (15%) sets as follows:</p> <pre><code>def stratified_split(y: pd.Series, n_bins: int = 10):\n    return pd.qcut(y, q=min(n_bins, y.nunique()), duplicates=\"drop\")\n\ny_bins = stratified_split(df['quality'], n_bins=10)\n\ntrain_df, test_df = train_test_split(\n    df,\n    test_size=0.15,\n    random_state=RANDOM_SEED,\n    stratify=y_bins,\n)\n\ny_bins_train = stratified_split(train_df['quality'], n_bins=10)\n\ntrain_df, val_df = train_test_split(\n    train_df,\n    test_size=0.1765,\n    random_state=RANDOM_SEED,\n    stratify=y_bins_train,\n)\n</code></pre>"},{"location":"regression-project/regression-project/#outlier-treatment","title":"Outlier Treatment","text":"<p>Although the dataset is clean, several features exhibit long right tails. The clipping of outliers was performed using the interquartile range (IQR) method, calculated only in the training set to prevent information leakage. This operation ensures that extreme values do not dominate the scale of each feature, while preserving the overall distribution and shape. </p> <pre><code>class OutlierClipper(BaseEstimator, TransformerMixin):\n    def __init__(self, factor: float = 3.0):\n        self.factor = factor\n        self.lower_ = None\n        self.upper_ = None\n        self.columns_ = None\n\n    def fit(self, X, y=None):\n        if isinstance(X, pd.DataFrame):\n            self.columns_ = X.columns\n            q1 = X.quantile(0.25)\n            q3 = X.quantile(0.75)\n        else:\n            X = pd.DataFrame(X)\n            self.columns_ = X.columns\n            q1 = X.quantile(0.25)\n            q3 = X.quantile(0.75)\n\n        iqr = q3 - q1\n        self.lower_ = q1 - self.factor * iqr\n        self.upper_ = q3 + self.factor * iqr\n        return self\n\n    def transform(self, X):\n        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self.columns_)\n        X_clipped = X_df.clip(self.lower_, self.upper_, axis=1)\n        return X_clipped.values\n</code></pre>"},{"location":"regression-project/regression-project/#normalization-and-encoding","title":"Normalization and Encoding","text":"<p>The final preprocessing pipeline applied 2 separate transformations: </p> <ul> <li> <p>Numerical Features: outlier clipping (previously described) and standardization using z-score transformation, ensuring each feature has a mean of 0 and a standard deviation of 1.</p> </li> <li> <p>Categorical Features: one-hot encoding to convert the <code>type</code> feature into binary columns, allowing the model to interpret categorical data effectively.</p> </li> </ul> <p>The transformation was implemented using a Column Transformer with a numerical pipeline and a categorical pipeline, as follows:</p> <pre><code>numeric_pipeline = Pipeline(steps=[\n    (\"clip\", OutlierClipper(factor=3.0)),\n    (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", drop=None, sparse_output=False)),\n])\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", numeric_pipeline, numeric_cols),\n        (\"cat\", categorical_pipeline, categorical_cols),\n    ],\n    remainder=\"drop\",\n)\n</code></pre> <p>The final processed datasets were saved as CSV files, and the preprocessing pipeline was serialized using <code>joblib</code> for future use.</p>"},{"location":"regression-project/regression-project/#summary","title":"Summary","text":"<p>At the end of preprocessing, we obtained the following dataset sizes:</p> <p><code>Sizes -&gt; X_train: (3723, 13), X_val: (799, 13), X_test: (798, 13)</code></p> <p>This indicates that the training set contains 3723 samples with 13 features (after one-hot encoding), while the validation and test sets contain 799 and 798 samples respectively.</p> <p>The summary statistics of the processed training dataset are as follows:</p> <p></p> <p>As we can see, the numerical features have been standardized to have a mean of approximately 0 and a standard deviation of approximately 1, which reduced the impact of outliers and differences in scale, while the categorical features have been one-hot encoded. The target variable <code>quality</code> remains unchanged.</p>"},{"location":"regression-project/regression-project/#4-mlp-implementation","title":"4. MLP Implementation","text":""},{"location":"regression-project/regression-project/#overview_1","title":"Overview","text":"<p>For the project, a Multilayer Perceptron model was implemented from scratch using only Numpy, without relying on deep learning frameworks. The goal was to understand and control every stage of the computation, from initialization of weights and the forward pass to gradient computation, backpropagation and optimization. The neural network predicts the continuous wine quality score based on the processed features.</p>"},{"location":"regression-project/regression-project/#model-architecture","title":"Model Architecture","text":"<p>The chosen architecture consists of:</p> <ul> <li> <p>Input layer: 13 input features (after preprocessing)</p> </li> <li> <p>Hidden layers: 2 hidden layers with 128 and 64 neurons respectively, using ReLU or tanh activation functions to introduce non-linearity.</p> </li> <li> <p>Output layer: 1 neuron with a linear function to produce continuous output for regression.</p> </li> </ul> <p>The final layer does not apply an activation function, as regression tasks require unrestricted continuous outputs.</p>"},{"location":"regression-project/regression-project/#activation-functions","title":"Activation Functions","text":"<p>The network supports ReLU, tanh and sigmoid activations, implemented directly in Numpy:</p> <pre><code>def act_forward(z, kind: str):\n    if kind == \"relu\":   \n        return np.maximum(0.0, z)\n    if kind == \"tanh\":   \n        return np.tanh(z)\n    if kind == \"sigmoid\":\n        return 1.0 / (1.0 + np.exp(-z))\n    raise ValueError(\"activation must be relu/tanh/sigmoid\")\n\ndef act_backward(z, a, kind: str):\n    if kind == \"relu\":   \n        return (z &gt; 0).astype(z.dtype)\n    if kind == \"tanh\":   \n        return 1.0 - a*a\n    if kind == \"sigmoid\":\n        return a * (1.0 - a)\n    raise ValueError(\"activation must be relu/tanh/sigmoid\")\n\ndef parse_hidden(s: str) -&gt; list[int]:\n    return [int(x) for x in s.split(\",\")] if s else [64]\n</code></pre> <p>ReLU was selected as the default because of its empirical stability and faster convergence, but the other functions are available as arguments in the model constructor.</p>"},{"location":"regression-project/regression-project/#loss-functions-and-metrics","title":"Loss Functions and Metrics","text":"<p>3 loss functions were implemented for regression:</p> <ul> <li> <p>Mean Squared Error (MSE): standard loss for regression tasks</p> </li> <li> <p>Mean Absolute Error (MAE): robust to outliers</p> </li> <li> <p>Root Mean Squared Error (RMSE): interpretable in the same units as the target variable</p> </li> <li> <p>R2 Score: statistical measure of how well the predictions approximate the actual values</p> </li> </ul> <pre><code>def mae(y_true, y_pred):\n    return float(np.mean(np.abs(y_pred - y_true)))\n\ndef mse(y_true, y_pred):\n    return float(np.mean((y_pred - y_true)**2))\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(mse(y_true, y_pred)))\n\ndef r2_score(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - y_true.mean())**2)\n    return float(1.0 - ss_res / (ss_tot + 1e-12))\n</code></pre>"},{"location":"regression-project/regression-project/#backpropagation","title":"Backpropagation","text":"<p>Gradients for all layers were computed manually using the chain rule. The derivative of each activation function was implemented in the <code>act_backward</code> function. </p> <pre><code>def backward(self, grad_out, caches):\n    \"\"\"\n    Backprop gradients from grad_out (= dL/dA_L, shape (N,1))\n    Returns lists dW, db\n    \"\"\"\n    L = len(self.W)\n    dZ = grad_out\n    dW_list, db_list = [None]*L, [None]*L\n\n    for l in reversed(range(L)):\n        A_prev, Z, A = caches[l]\n        dW = A_prev.T @ dZ + self.l2 * self.W[l]\n        db = np.sum(dZ, axis=0, keepdims=True)\n\n        dW_list[l] = dW.astype(np.float32)\n        db_list[l] = db.astype(np.float32)\n\n        if l &gt; 0:\n            dA_prev = dZ @ self.W[l].T\n            A_prev_prev, Z_prev, A_prev_post = caches[l-1]\n            dZ = dA_prev * act_backward(Z_prev, A_prev_post, self.activation)\n\n    return dW_list, db_list\n</code></pre>"},{"location":"regression-project/regression-project/#optimization-and-regularization","title":"Optimization and Regularization","text":"<p>The optimizer uses mini-batch stochastic gradient descent (SGD) with a fixed learning rate. L2 regularization is appplied during each step update to prevent overfitting.</p> <ul> <li> <p>Learning Rate: 0.001</p> </li> <li> <p>Batch Size: 256</p> </li> <li> <p>L2 Regularization: 0.001</p> </li> <li> <p>Early stopping if validation loss does not improve for 20 consecutive epochs.</p> </li> </ul> <p>Here is the implemented training loop:</p> <pre><code>def train_loop(\n    model: MLPReg,\n    Xtr, ytr, Xva, yva,\n    epochs=200, lr=1e-3, batch_size=256,\n    loss=\"mse\", huber_delta=1.0,\n    seed=42, patience=20\n):\n    rng = np.random.default_rng(seed)\n    n = Xtr.shape[0]\n    best = {\"val_loss\": np.inf, \"W\": None, \"b\": None, \"epoch\": 0}\n    history = []\n\n    for epoch in range(1, epochs+1):\n        idx = rng.permutation(n)\n        for start in range(0, n, batch_size):\n            b = idx[start:start+batch_size]\n            y_pred, caches = model.forward(Xtr[b])\n            L, dY = loss_and_grad(ytr[b], y_pred, loss, huber_delta)\n            l2_term = 0.5 * model.l2 * sum((W**2).sum() for W in model.W)\n            L_total = L + (l2_term / n)\n\n            dW, db = model.backward(dY, caches)\n            model.step(dW, db, lr)\n\n        ytr_pred = model.predict(Xtr)\n        yva_pred = model.predict(Xva)\n\n        tr_L, _ = loss_and_grad(ytr, ytr_pred, loss, huber_delta)\n        va_L, _ = loss_and_grad(yva, yva_pred, loss, huber_delta)\n\n        tr_mae = mae(ytr, ytr_pred); va_mae = mae(yva, yva_pred)\n        tr_rmse = rmse(ytr, ytr_pred); va_rmse = rmse(yva, yva_pred)\n        tr_r2 = r2_score(ytr, ytr_pred); va_r2 = r2_score(yva, yva_pred)\n\n        history.append({\n            \"epoch\": epoch,\n            \"train_loss\": tr_L, \"val_loss\": va_L,\n            \"train_mae\": tr_mae, \"val_mae\": va_mae,\n            \"train_rmse\": tr_rmse, \"val_rmse\": va_rmse,\n            \"train_r2\": tr_r2, \"val_r2\": va_r2,\n        })\n\n        print(\n            f\"epoch {epoch:03d} | \"\n            f\"tr {loss} {tr_L:.4f}  mae {tr_mae:.4f} rmse {tr_rmse:.4f} r2 {tr_r2:.4f} | \"\n            f\"va {loss} {va_L:.4f}  mae {va_mae:.4f} rmse {va_rmse:.4f} r2 {va_r2:.4f}\"\n        )\n\n        if va_L + 1e-9 &lt; best[\"val_loss\"]:\n            best[\"val_loss\"] = va_L\n            best[\"epoch\"] = epoch\n            best[\"W\"] = [W.copy() for W in model.W]\n            best[\"b\"] = [b.copy() for b in model.b]\n        elif epoch - best[\"epoch\"] &gt;= patience:\n            print(f\"Early stopping at epoch {epoch}, best epoch {best['epoch']} (val_loss={best['val_loss']:.4f})\")\n            break\n\n    if best[\"W\"] is not None:\n        model.W = best[\"W\"]\n        model.b = best[\"b\"]\n\n    return history\n\n# Main Helpers\ndef load_xy(path_csv: str, target_col: str = \"quality\"):\n    df = pd.read_csv(path_csv)\n    assert target_col in df.columns, f\"{target_col} not found in {path_csv}\"\n    y = df[target_col].to_numpy(dtype=np.float32).reshape(-1, 1)\n    X = df.drop(columns=[target_col]).to_numpy(dtype=np.float32)\n    return X, y, df.columns.tolist()\n\ndef save_history(history, out_path: Path):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, \"w\") as f:\n        json.dump(history, f, indent=2)\n\ndef save_weights(model: MLPReg, out_path: Path):\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    np.savez(out_path, **{f\"W{i}\": W for i, W in enumerate(model.W)},\n                      **{f\"b{i}\": b for i, b in enumerate(model.b)})\n</code></pre>"},{"location":"regression-project/regression-project/#main-execution","title":"Main Execution","text":"<p>Finally, the model module also has a main execution block with argument parsing to run training and evaluation from the command line.</p> <pre><code>def main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--train\", type=str, default=\"data/processed/train.csv\")\n    ap.add_argument(\"--valid\", type=str, default=\"data/processed/valid.csv\")\n    ap.add_argument(\"--test\",  type=str, default=\"data/processed/test.csv\")\n    ap.add_argument(\"--hidden\", type=str, default=\"128,64\")\n    ap.add_argument(\"--activation\", type=str, default=\"relu\", choices=[\"relu\",\"tanh\",\"sigmoid\"])\n    ap.add_argument(\"--loss\", type=str, default=\"mse\", choices=[\"mse\",\"mae\",\"huber\"])\n    ap.add_argument(\"--huber_delta\", type=float, default=1.0)\n    ap.add_argument(\"--epochs\", type=int, default=200)\n    ap.add_argument(\"--lr\", type=float, default=1e-3)\n    ap.add_argument(\"--batch_size\", type=int, default=256)\n    ap.add_argument(\"--patience\", type=int, default=20)\n    ap.add_argument(\"--l2\", type=float, default=1e-4)\n    ap.add_argument(\"--seed\", type=int, default=42)\n    ap.add_argument(\"--outdir\", type=str, default=\"outputs\")\n    args = ap.parse_args()\n\n    outdir = Path(args.outdir)\n    outdir.mkdir(parents=True, exist_ok=True)\n\n    # Save run config\n    with open(outdir / \"config.json\", \"w\") as f:\n        json.dump(vars(args), f, indent=2)\n\n    # Load splits\n    Xtr, ytr, _ = load_xy(args.train)\n    Xva, yva, _ = load_xy(args.valid)\n    Xte, yte, _ = load_xy(args.test)\n\n    # Init model\n    model = MLPReg(\n        input_dim=Xtr.shape[1],\n        hidden=parse_hidden(args.hidden),\n        activation=args.activation,\n        seed=args.seed,\n        l2=args.l2\n    )\n\n    # Train\n    history = train_loop(\n        model, Xtr, ytr, Xva, yva,\n        epochs=args.epochs, lr=args.lr, batch_size=args.batch_size,\n        loss=args.loss, huber_delta=args.huber_delta,\n        seed=args.seed, patience=args.patience\n    )\n\n    # Save history (JSON + CSV)\n    save_history(history, outdir / \"history.json\")\n    try:\n        pd.DataFrame(history).to_csv(outdir / \"history.csv\", index=False)\n    except Exception as e:\n        print(\"Warning: failed to save history.csv:\", e)\n\n    # Save weights\n    save_weights(model, outdir / \"weights.npz\")\n\n    # Evaluate on test\n    yte_pred = model.predict(Xte)\n    test_loss, _ = loss_and_grad(yte, yte_pred, args.loss, args.huber_delta)\n    test_mae = mae(yte, yte_pred)\n    test_mse = mse(yte, yte_pred)\n    test_rmse = rmse(yte, yte_pred)\n    test_r2 = r2_score(yte, yte_pred)\n\n    metrics = {\n        \"loss\": args.loss,\n        \"huber_delta\": args.huber_delta if args.loss == \"huber\" else None,\n        \"test_loss\": float(test_loss),\n        \"test_mae\": float(test_mae),\n        \"test_mse\": float(test_mse),\n        \"test_rmse\": float(test_rmse),\n        \"test_r2\": float(test_r2),\n    }\n    with open(outdir / \"metrics.json\", \"w\") as f:\n        json.dump(metrics, f, indent=2)\n\n    # Save predictions &amp; residuals\n    pred_df = pd.DataFrame({\n        \"y_true\": yte.reshape(-1),\n        \"y_pred\": yte_pred.reshape(-1),\n    })\n    pred_df[\"residual\"] = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n    pred_df.to_csv(outdir / \"predictions_test.csv\", index=False)\n\n    print(\n        f\"\\nTest results \u2192 {args.loss}: {test_loss:.4f} | \"\n        f\"MAE: {test_mae:.4f} | RMSE: {test_rmse:.4f} | R2: {test_r2:.4f}\"\n    )\n    print(f\"Artifacts saved to: {outdir.resolve()}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"regression-project/regression-project/#5-model-training","title":"5. Model Training","text":""},{"location":"regression-project/regression-project/#overview_2","title":"Overview","text":"<p>This section presents the training process of the MLP model implemented previously. The model was trained on the preprocessed dataset, using the functions and classes described above. The training procedure implements all essential components of a supervised regression workflow, including data loading, model initialization, training loop, validation, and evaluation on the test set.</p>"},{"location":"regression-project/regression-project/#training-configuration","title":"Training Configuration","text":"<p>The model was trained with the following hyperparameters:</p> Hyperparameter Value Description Hidden layers <code>[128, 64]</code> Two hidden layers for feature abstraction Activation ReLU Stable and efficient for deep architectures Loss Function MSE Penalizes large errors more heavily Learning Rate 0.001 Balanced convergence speed and stability Batch Size 256 Mini-batch gradient descent Epochs 1000 Upper bound for early stopping Patience 20 Stops if validation loss stagnates Regularization (L2) 1e-4 Prevents overfitting by penalizing large weights Seed 42 Reproducibility <p>The training loop uses mini-batch SGD. At each iteration, the network performs:</p> <ol> <li> <p>Forward pass to compute predictions</p> </li> <li> <p>Loss computation using MSE</p> </li> <li> <p>Backward pass to compute gradients</p> </li> <li> <p>Parameter updates with L2 regularization</p> </li> <li> <p>Validation step to monitor performance for early stopping</p> </li> </ol> <p>The process repeats until the validation loss no longer improves for 20 epochs, at which point training halts to prevent overfitting.</p> <p>Here is the complete training implementation:</p> <pre><code>config = {\n    \"hidden\": \"128,64\",\n    \"activation\": \"relu\",\n    \"loss\": \"mse\",\n    \"huber_delta\": 1.0,\n    \"epochs\": 1000,\n    \"lr\": 1e-3,\n    \"batch_size\": 256,\n    \"patience\": 20,\n    \"l2\": 1e-4,\n    \"seed\": SEED\n}\n\nmodel = MLPReg(\n    input_dim=Xtr.shape[1],\n    hidden=parse_hidden(config[\"hidden\"]),\n    activation=config[\"activation\"],\n    seed=config[\"seed\"],\n    l2=config[\"l2\"]\n)\n\nhistory = train_loop(\n    model, Xtr, ytr, Xva, yva,\n    epochs=config[\"epochs\"],\n    lr=config[\"lr\"],\n    batch_size=config[\"batch_size\"],\n    loss=config[\"loss\"],\n    huber_delta=config[\"huber_delta\"],\n    seed=config[\"seed\"],\n    patience=config[\"patience\"],\n)\n</code></pre> <p>The results and weights are saved for future analysis and evaluation.</p>"},{"location":"regression-project/regression-project/#training-dynamics","title":"Training Dynamics","text":"<p>After preprocessing, the model received 13 features and produced a single continuous output. The MLP did not use early stopping for the 1000 epochs, as the validation loss continued to improve until the last epoch. The training and validation losses decreased steadily, indicating effective learning without overfitting.</p> <p>Excerpt from training log:</p> <pre><code>epoch 997 | tr mse 0.4201  mae 0.5069 rmse 0.6481 r2 0.4538 | va mse 0.4872  mae 0.5400 rmse 0.6980 r2 0.3638\nepoch 998 | tr mse 0.4199  mae 0.5070 rmse 0.6480 r2 0.4540 | va mse 0.4871  mae 0.5401 rmse 0.6979 r2 0.3639\nepoch 999 | tr mse 0.4199  mae 0.5070 rmse 0.6480 r2 0.4541 | va mse 0.4873  mae 0.5401 rmse 0.6980 r2 0.3637\nepoch 1000 | tr mse 0.4198  mae 0.5069 rmse 0.6479 r2 0.4541 | va mse 0.4872  mae 0.5400 rmse 0.6980 r2 0.3638\n</code></pre> <p>The validation loss at the end of training was approximately 0.4872 (MSE), with a corresponding RMSE of 0.6980 and R2 score of 0.3638, indicating moderate predictive performance.</p> <p>After that, we tested the model on the test set, obtaining the following results:</p> <ul> <li> <p>MSE: 0.4293</p> </li> <li> <p>MAE: 0.5161</p> </li> <li> <p>RMSE: 0.6552</p> </li> <li> <p>R2 Score: 0.4102</p> </li> </ul> <p>These results suggest that the model generalizes reasonably well to unseen data, maintaining performance similar to that observed on the validation set. The moderate R2 score indicates that while the model captures some of the variance in wine quality, there is still room for improvement, potentially through hyperparameter tuning, architecture adjustments, or additional feature engineering.</p> <p>Evaluation metrics were computed and as follows:</p> <pre><code>def mae(y_true, y_pred):\n    return float(np.mean(np.abs(y_pred - y_true)))\n\ndef mse(y_true, y_pred):\n    return float(np.mean((y_pred - y_true)**2))\n\ndef rmse(y_true, y_pred):\n    return float(np.sqrt(mse(y_true, y_pred)))\n\ndef r2_score(y_true, y_pred):\n    ss_res = np.sum((y_true - y_pred)**2)\n    ss_tot = np.sum((y_true - y_true.mean())**2)\n    return float(1.0 - ss_res / (ss_tot + 1e-12))\n</code></pre>"},{"location":"regression-project/regression-project/#6-training-and-testing-strategy","title":"6. Training and Testing Strategy","text":""},{"location":"regression-project/regression-project/#data-splitting_1","title":"Data Splitting","text":"<p>The dataset was divided into 70 % training, 15 % validation, and 15 % testing using NumPy\u2019s random permutation with a fixed seed (42) for reproducibility.</p> <ul> <li>Training set: used to learn model parameters.</li> <li>Validation set: used to monitor performance during training, tune hyperparameters, and trigger early stopping.</li> <li>Test set: kept completely unseen until the final evaluation to provide an unbiased measure of generalization.</li> </ul> <p>This 70/15/15 ratio gives a good balance\u2014enough data for training while reserving sufficient samples to validate and test reliability.</p>"},{"location":"regression-project/regression-project/#training-mode","title":"Training Mode","text":"<p>We used mini-batch gradient descent because it offers a practical compromise between:</p> <ul> <li>Speed (faster than stochastic = batch size 1) and</li> <li>Stability (less noisy than full-batch).</li> </ul> <p>A batch size of 256 allowed efficient vectorized computation and smooth convergence of the loss curve. Each mini-batch updates the model weights, ensuring steady progress without overreacting to individual samples.</p>"},{"location":"regression-project/regression-project/#early-stopping-to-prevent-overfitting","title":"Early Stopping to Prevent Overfitting","text":"<p>During training we track the validation loss after each epoch. If it fails to improve for several epochs (patience = 20), training stops and the weights from the best validation epoch are restored. This prevents the network from memorizing noise in the training set and preserves generalization.</p> <p>Early stopping is a simple but effective form of regularization\u2014our model consistently stopped before the validation loss started to rise.</p>"},{"location":"regression-project/regression-project/#reproducibility-and-validation-role","title":"Reproducibility and Validation Role","text":"<p>All experiments use random seed = 42, fixed across data splitting, initialization, and shuffling, ensuring identical results on reruns. Validation loss drives decisions such as:</p> <ul> <li>tuning hidden-layer sizes, learning rate, and batch size;</li> <li>deciding when to halt training;</li> <li>and selecting the final model checkpoint.</li> </ul> <p>Because hyperparameters are chosen solely based on validation performance, the test set remains untouched until the very end, guaranteeing an honest evaluation.</p>"},{"location":"regression-project/regression-project/#summary_1","title":"Summary","text":"<p>Our strategy can be summarized as:</p> Aspect Choice Rationale Split ratio 70 / 15 / 15 Balance between training data and reliable validation/testing Batch size 256 Fast and stable learning Training mode Mini-batch GD Efficient compromise between speed and accuracy Early stopping Patience = 20 Avoids overfitting Random seed 42 Ensures reproducibility Validation use Hyperparameter tuning &amp; checkpoint selection Prevents test leakage"},{"location":"regression-project/regression-project/#7-error-curves-and-visualization","title":"7. Error Curves and Visualization","text":"<p>The error curves help evaluate how well the model learns and generalizes across training and validation data. We plotted both the loss (MSE) and the coefficient of determination (R\u00b2) over epochs to analyze convergence, stability, and overfitting behavior.</p>"},{"location":"regression-project/regression-project/#71-training-and-validation-loss-curves","title":"7.1 Training and Validation Loss Curves","text":"<p>Interpretation: Both curves show a steep decrease in the first ~50\u2013100 epochs, followed by a gradual flattening around epoch 200, where both training and validation losses stabilize. This indicates that:</p> <ul> <li>The model converged successfully, reaching minimal error values.</li> <li>There is no clear overfitting, since validation loss follows the training loss closely.</li> <li>The nearly overlapping curves show excellent generalization \u2014 the model performs similarly on unseen data.</li> </ul> <p>In summary, the model converged smoothly with minimal gap between training and validation losses, demonstrating balanced learning and effective regularization.</p>"},{"location":"regression-project/regression-project/#72-training-and-validation-r2-curves","title":"7.2 Training and Validation R\u00b2 Curves","text":"<p>Interpretation: R\u00b2 measures how much variance in the target variable the model explains (1.0 = perfect prediction). Here, both training and validation R\u00b2 values rise sharply during the first 100 epochs, then gradually approach values near 0.9\u20131.0 around epoch 200, where they plateau.</p> <ul> <li>The parallel, overlapping trends confirm that the model generalizes well and avoids overfitting.</li> <li>The early negative R\u00b2 values are expected, meaning that at the beginning the model performed worse than a mean predictor, but quickly improved.</li> <li>The final plateau confirms stable learning and that the model has captured most of the variance in the target data.</li> </ul> <p>These curves demonstrate consistent convergence and strong predictive power across both splits.</p>"},{"location":"regression-project/regression-project/#73-discussion-of-trends","title":"7.3 Discussion of Trends","text":"<ul> <li>Convergence: Loss decreased and R\u00b2 increased steadily before stabilizing \u2014 the model reached an optimal state.</li> <li>Overfitting: No strong divergence between train and validation metrics, confirming early stopping and L2 regularization worked effectively.</li> <li>Underfitting: None detected, both metrics reached strong values, confirming adequate model capacity and hyperparameter tuning.</li> </ul>"},{"location":"regression-project/regression-project/#74-convergence-overfitting-underfitting-and-adjustments-made","title":"7.4 Convergence, Overfitting / Underfitting, and Adjustments Made","text":""},{"location":"regression-project/regression-project/#did-the-model-converge","title":"Did the model converge?","text":"<p>The model converged when the validation loss stopped improving around ~200 epochs. After that, performance stabilized, showing diminishing returns from additional training.</p>"},{"location":"regression-project/regression-project/#did-we-overfit","title":"Did we overfit?","text":"<p>We monitored overfitting by observing:</p> <ul> <li>training loss continuing to decrease,</li> <li>validation loss plateauing or rising, and</li> <li>validation R\u00b2 leveling off.   This behavior marked the start of overfitting, which was controlled via early stopping (patience = 20) \u2014 ensuring the best validation checkpoint was restored instead of the final epoch.</li> </ul> <p>So, no significant underfitting was observed. After tuning the model architecture, learning rate, and batch size, both losses reached low stable values and R\u00b2 approached 1.0, indicating good learning capacity.</p>"},{"location":"regression-project/regression-project/#what-we-changed-because-of-these-curves","title":"What we changed because of these curves","text":"Observation from curves Adjustment made Why it helps Validation loss plateaued / started to rise Added early stopping (patience = 20) Prevents overfitting by restoring the best epoch Slight train/val gap growing Applied L2 regularization (weight decay) Improves generalization and reduces variance Small initial instability in loss curve Chose mini-batch size = 256 Smoother, more stable gradient updates"},{"location":"regression-project/regression-project/#75-conclusion","title":"7.5 Conclusion","text":"<p>We plotted training and validation loss over epochs and observed that both decreased rapidly during early training, then the validation loss plateaued. After approximately 200 epochs, the validation loss stopped improving while the training loss continued to decrease, indicating the onset of overfitting. We also tracked R\u00b2 (explained variance) for both training and validation splits: validation R\u00b2 rose steadily before leveling off, confirming that generalization peaked at that point. Based on these curves, we implemented early stopping (patience = 20) and restored the model weights from the epoch with the lowest validation loss. We also adopted mini-batch training (batch size = 256) and light L2 regularization, which stabilized training and reduced overfitting. These diagnostic plots directly informed our hyperparameter choices and justify the final model checkpoint used for evaluation in Section 8.</p>"},{"location":"regression-project/regression-project/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"regression-project/regression-project/#81-numerical-results","title":"8.1 Numerical Results","text":"Metric Value Interpretation MAE 0.4626 On average, the model\u2019s predictions are off by less than half a quality point \u2014 quite good for a 0\u201310 scale. MSE 0.5200 Moderate squared error; penalizes larger deviations more strongly than MAE. RMSE 0.7211 Roughly the standard deviation of prediction errors, meaning most predictions are within \u00b10.7 points. R\u00b2 0.2857 The model explains about 29% of the variance in wine quality \u2014 modest but clearly better than random or mean predictions. <p>Baseline (mean predictor):</p> <ul> <li>MAE = 0.6709, RMSE = 0.8532, R\u00b2 = 0.0000   \u2192 The baseline simply predicts the average quality for every wine and captures no variance.   Our MLP clearly outperforms this baseline, confirming that it learned meaningful relationships.</li> </ul> <p>Performance Discussion</p> <ul> <li>The R\u00b2 of 0.29 is consistent with typical results for the Wine Quality dataset \u2014 the data has high noise and overlapping quality classes, so explaining even 25\u201335% of variance is expected.</li> <li>MAE below 0.5 indicates solid predictive precision, roughly within half a quality point of the truth on average.</li> <li>RMSE &gt; MAE is normal, showing a few slightly larger errors, but not extreme outliers.</li> <li>Compared to the mean predictor, the error reduction (~30%) demonstrates real learning beyond the average baseline.</li> </ul> <p>Error Analysis &amp; Residuals</p> <p>From the residual plots:</p> <ul> <li>The residuals are centered around 0, showing no systematic over- or under-prediction bias.</li> <li>The scatter appears random, suggesting the model generalizes reasonably well \u2014 no clear trend of errors increasing for certain predicted values.</li> <li>The histogram of residuals is approximately symmetric, meaning errors are balanced on both sides of the true value.</li> </ul> <p>Strengths &amp; Weaknesses</p> Strength Explanation Low average prediction error MAE \u2248 0.46 is good for subjective rating data. Balanced residuals Indicates stable learning and no consistent bias. Beats baseline Clear improvement over mean predictor baseline. Weakness Explanation Moderate R\u00b2 Model doesn\u2019t capture all variability \u2014 some randomness in labels remains. Slight noise sensitivity RMSE &gt; MAE suggests a few higher-error samples. <p>Conclusion of Numerical Results</p> <p>The final MLP regression model achieved an MAE of 0.4626, RMSE of 0.7211, and R\u00b2 of 0.2857 on the test set, clearly outperforming the mean predictor baseline (MAE = 0.6709, RMSE = 0.8532, R\u00b2 = 0.0). These results show that the model effectively captures meaningful nonlinear relationships between input features and wine quality ratings. While the explained variance remains moderate, the model demonstrates consistent, unbiased predictions with small errors, which aligns with expectations for this dataset\u2019s inherent subjectivity and noise. Residual analyses confirmed no significant bias, indicating good generalization. Overall, the trained model provides a reasonable balance between accuracy and interpretability for this regression task.</p>"},{"location":"regression-project/regression-project/#82-residual-analysis","title":"8.2 Residual Analysis","text":"<p>Residual plots help us visualize the model\u2019s bias and error distribution: - Residuals close to 0 \u2192 unbiased predictions. - Random scatter \u2192 no systematic bias. - Wide or patterned scatter \u2192 heteroscedasticity or model misfit.</p> <p>We generated two key residual plots:</p>"},{"location":"regression-project/regression-project/#residual-plot-interpretation-residuals-vs-predicted","title":"Residual Plot Interpretation (Residuals vs Predicted)","text":"<p>Visual Behavior</p> <ul> <li>The horizontal red dashed line at 0 represents perfect predictions (no error).</li> <li>Each point shows the residual (True \u2013 Predicted) for a test sample at its predicted quality value.</li> </ul> <p>Observations</p> <ol> <li> <p>Residuals cluster around 0:    Most points lie close to the zero line, indicating that predictions are centered and unbiased on average.</p> </li> <li> <p>No clear trend across predicted quality:    The residuals appear randomly distributed for predicted values (4\u20137), showing that the model does not systematically over- or under-predict at any particular quality level.</p> </li> <li> <p>Discrete residual values:    Because wine quality ratings are integer-based (1\u201310), both predictions and actuals fall into discrete bins, creating visible \u201cstripes.\u201d This is normal and not a model defect.</p> </li> <li> <p>No major outliers:    There are no extreme deviations (e.g., residuals &gt; \u00b13). The largest residuals are around \u20133 and +2, which are acceptable considering the rating scale.</p> </li> </ol> <p>Interpretation</p> <p>This plot confirms that:</p> <ul> <li>The model\u2019s predictions are fairly well-calibrated \u2014 errors fluctuate symmetrically around zero.</li> <li>There\u2019s no heteroscedasticity (variance of errors remains constant across the prediction range).</li> <li>The MLP model generalizes consistently across all predicted quality levels without major bias.</li> </ul> <p>Conclusion</p> <p>The residual plot shows that prediction errors are centered near zero, with no clear upward or downward trend, indicating that the model\u2019s predictions are unbiased and stable across the quality range. The residuals form discrete bands due to the integer nature of wine quality scores. The absence of large outliers or funnel-shaped patterns suggests constant variance of errors (homoscedasticity), implying that the model generalizes consistently across all predicted values.</p>"},{"location":"regression-project/regression-project/#residuals-distributiontest-set-interpretation","title":"Residuals Distribution(Test Set) Interpretation","text":"<p>Visual Behavior</p> <ul> <li>The plot shows how residuals (True \u2013 Predicted) are distributed across the test samples.</li> <li>The red dashed line at 0 indicates perfect predictions (no error).</li> </ul> <p>Observations</p> <ol> <li> <p>Strong central peak near 0:    The majority of residuals fall around 0, confirming that the model\u2019s predictions are well-centered and unbiased overall.</p> </li> <li> <p>Symmetrical distribution:    The histogram is approximately symmetric, with residuals equally likely to be slightly positive or slightly negative.    \u2192 This means the model doesn\u2019t systematically overpredict or underpredict.</p> </li> <li> <p>Small tails on both sides:    A few samples deviate by \u00b12 to \u00b13, but such outliers are rare.    \u2192 Suggests stable generalization and no extreme mispredictions.</p> </li> <li> <p>Discrete bins:    The \u201cbar-like\u201d distribution arises because the dataset labels are integer quality scores, and predictions were rounded or quantized, resulting in discrete residual values.</p> </li> </ol> <p>Interpretation</p> <ul> <li> <p>The residuals follow a near-normal, zero-centered pattern, indicating:</p> </li> <li> <p>Low bias</p> </li> <li>Consistent prediction spread</li> <li>Minimal outliers</li> <li>This confirms that the MLP regressor generalizes well without skewing predictions toward high or low quality values.</li> </ul> <p>Conclusion</p> <p>The residual distribution shows a strong peak around zero, indicating that the model\u2019s predictions are unbiased overall. The residuals are approximately symmetric, with small counts for larger positive and negative errors, suggesting stable generalization. The discrete bar structure reflects the integer nature of wine quality ratings. The lack of long tails or heavy skew confirms that the model rarely makes large errors, and its prediction noise is well-balanced around the true values.</p>"},{"location":"regression-project/regression-project/#general-interpretation-and-conclusion","title":"General Interpretation and Conclusion","text":""},{"location":"regression-project/regression-project/#83-interpretation","title":"8.3 Interpretation","text":"<ul> <li>Moderate MAE / RMSE: The model\u2019s predictions are close to true values on average (MAE \u2248 0.46, RMSE \u2248 0.72). The small gap between MAE and RMSE indicates that large outliers are rare.</li> <li>Reasonable R\u00b2 (\u2248 0.29): The MLP explains roughly 29% of the variance in wine quality \u2014 consistent with expectations for this noisy, subjective dataset.</li> <li>Baseline Comparison: The MLP clearly outperforms the mean predictor (MAE 0.67 \u2192 0.46, RMSE 0.85 \u2192 0.72, R\u00b2 0.00 \u2192 0.29), confirming that the model learned meaningful relationships rather than simply guessing the average.</li> <li>Residuals: Residuals are centered near zero and show no clear pattern or trend, demonstrating that the model is unbiased and generalizes consistently across the prediction range.</li> </ul>"},{"location":"regression-project/regression-project/#84-conclusion","title":"8.4 Conclusion","text":"<p>The final MLP regressor achieved solid predictive accuracy, with an MAE of approximately 0.46, RMSE of 0.72, and R\u00b2 of 0.29 on the held-out test set. These results indicate that the model captures meaningful nonlinear patterns while maintaining generalization. Compared to a mean predictor baseline, it reduced both MAE and RMSE by about 30%, confirming genuine learning. Residual analyses revealed well-centered, symmetric error distributions without significant bias or heteroscedasticity. Overall, the model demonstrates reliable performance and balanced generalization for this regression task, effectively predicting wine quality within about half a quality point on average.</p> <p>Note: AI assistance was used for code scaffolding, documentation, and figure generation. The authors understand and can explain all parts of the solution; plagiarism policies were respected.</p>"},{"location":"roteiro1/main/","title":"Main","text":""},{"location":"roteiro1/main/#objetivo","title":"Objetivo","text":"<p>Aqui vai o objetivo macro do roteiro. Por que estamos fazendo o que estamos fazendo?</p>"},{"location":"roteiro1/main/#montagem-do-roteiro","title":"Montagem do Roteiro","text":"<p>Os pontos \"tarefas\" s\u00e3o os passos que devem ser seguidos para a realiza\u00e7\u00e3o do roteiro. Eles devem ser claros e objetivos. Com evid\u00eancias claras de que foram realizados.</p>"},{"location":"roteiro1/main/#tarefa-1","title":"Tarefa 1","text":"<p>Instalando o MAAS:</p> sudo snap install maas --channel=3.5/Stable <p></p> <p>Dashboard do MAAS</p> <p>Conforme ilustrado acima, a tela inicial do MAAS apresenta um dashboard com informa\u00e7\u00f5es sobre o estado atual dos servidores gerenciados. O dashboard \u00e9 composto por diversos pain\u00e9is, cada um exibindo informa\u00e7\u00f5es sobre um aspecto espec\u00edfico do ambiente gerenciado. Os pain\u00e9is podem ser configurados e personalizados de acordo com as necessidades do usu\u00e1rio.</p>"},{"location":"roteiro1/main/#tarefa-2","title":"Tarefa 2","text":""},{"location":"roteiro1/main/#app","title":"App","text":""},{"location":"roteiro1/main/#tarefa-1_1","title":"Tarefa 1","text":""},{"location":"roteiro1/main/#tarefa-2_1","title":"Tarefa 2","text":"<p>Exemplo de diagrama</p> <pre><code>architecture-beta\n    group api(cloud)[API]\n\n    service db(database)[Database] in api\n    service disk1(disk)[Storage] in api\n    service disk2(disk)[Storage] in api\n    service server(server)[Server] in api\n\n    db:L -- R:server\n    disk1:T -- B:server\n    disk2:T -- B:db</code></pre> <p>Mermaid</p>"},{"location":"roteiro1/main/#questionario-projeto-ou-plano","title":"Question\u00e1rio, Projeto ou Plano","text":"<p>Esse se\u00e7\u00e3o deve ser preenchida apenas se houver demanda do roteiro.</p>"},{"location":"roteiro1/main/#discussoes","title":"Discuss\u00f5es","text":"<p>Quais as dificuldades encontradas? O que foi mais f\u00e1cil? O que foi mais dif\u00edcil?</p>"},{"location":"roteiro1/main/#conclusao","title":"Conclus\u00e3o","text":"<p>O que foi poss\u00edvel concluir com a realiza\u00e7\u00e3o do roteiro?</p>"},{"location":"roteiro2/main/","title":"Main","text":""},{"location":"roteiro2/main/#diagrama-de-classes-do-banco","title":"Diagrama de Classes do Banco","text":"<pre><code>classDiagram\n    class Conta {\n        - String id\n        # double saldo\n        - Cliente cliente\n        + sacar(double valor)\n        + depositar(double valor)\n    }\n    class Cliente {\n        - String id\n        - String nome\n        - List&lt;Conta&gt; contas\n    }\n    class PessoaFisica {\n        - String cpf\n    }\n    class PessoaJuridica {\n        - String cnpj\n    }\n    class ContaCorrente {\n        - double limite\n        + sacar(double valor)\n    }\n    class ContaPoupanca {\n        + sacar(double valor)\n    }\n    Conta *-- Cliente\n    Conta &lt;|-- ContaCorrente\n    Conta &lt;|-- ContaPoupanca\n    Cliente &lt;|-- PessoaFisica\n    Cliente &lt;|-- PessoaJuridica</code></pre>"},{"location":"roteiro2/main/#diagrama-de-sequencia-de-autorizacao","title":"Diagrama de Seq\u00fc\u00eancia de Autoriza\u00e7\u00e3o","text":"<pre><code>sequenceDiagram\n  autonumber\n  actor User\n  User-&gt;&gt;Auth Service: request with token\n  Auth Service-&gt;&gt;Auth Service: decodes the token and extracts claims\n  Auth Service-&gt;&gt;Auth Service: verifies permissions\n  critical allowed\n    Auth Service-&gt;&gt;Secured Resource: authorizes the request\n    Secured Resource-&gt;&gt;User: returns the response\n  option denied\n    Auth Service--&gt;&gt;User: unauthorized message\n  end  </code></pre>"},{"location":"roteiro3/main/","title":"Main","text":"<p>Running the code below in Browser (Woooooowwwwww!!!!!!). <sup>1</sup></p> <p> </p> Editor (session: default) Run <pre>import ssl\nimport pandas as pd\n\ndf = pd.DataFrame()\ndf['AAPL'] = pd.Series([1, 2, 3])\ndf['MSFT'] = pd.Series([4, 5, 6])\ndf['GOOGL'] = pd.Series([7, 8, 9])\n\nprint(df)\n</pre> Output Clear <pre></pre> <p></p> <ol> <li> <p>Pyodide \u21a9</p> </li> </ol>"},{"location":"roteiro4/main/","title":"Main","text":"<p>Se chegou aqui, \u00e9 porque voc\u00ea est\u00e1 interessado em saber mais. Logo, de brinde, como rodar um c\u00f3digo <code>Python</code> aqui.</p> 2025-10-29T09:14:34.465472 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ 2025-10-29T09:14:36.764283 image/svg+xml Matplotlib v3.10.7, https://matplotlib.org/ <p>Markdown-exec \u00e9 uma extens\u00e3o do Markdown que permite executar c\u00f3digo Python diretamente no Markdown. Isso \u00e9 \u00fatil para gerar resultados din\u00e2micos ou executar scripts de forma interativa.</p>"},{"location":"thisdocumentation/main/","title":"Main","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"},{"location":"vae/vae-exercise/","title":"VAE Task","text":""},{"location":"vae/vae-exercise/#variational-autoencoder-task","title":"Variational Autoencoder Task","text":"<p>The complete source code for this task is available here</p>"},{"location":"vae/vae-exercise/#data-preparation","title":"Data Preparation","text":"<p>Before training the VAE, we first need to prepare the Fashion MNIST Dataset. </p>"},{"location":"vae/vae-exercise/#1-loading-the-dataset","title":"1. Loading the Dataset","text":"<p>We use Pytorch's <code>torchvision.datasets.FashionMNIST</code> to automatically download the dataset. It contains 60k training and 10k test grayscale images of clothing items. </p>"},{"location":"vae/vae-exercise/#2-normalization","title":"2. Normalization","text":"<p>Each image is converted into a tensor and normalized to the range \\([0,1]\\) using <code>transforms.ToTensor()</code>. </p>"},{"location":"vae/vae-exercise/#3-train-validation-split","title":"3. Train-validation Split","text":"<p>To monitor generalization, we split the training set into train and validation subsets. A 10% validation split is applied using <code>random_split</code>. </p>"},{"location":"vae/vae-exercise/#4-data-loader","title":"4. Data Loader","text":"<p>We wrap the datasets into Pytorch Dataloaders, which handles batching and shuffling automatically. The parameters used are:</p> <ul> <li> <p>Batch Size = 128</p> </li> <li> <p>Shuffle = True</p> </li> <li> <p>Number of Workers = 4</p> </li> </ul>"},{"location":"vae/vae-exercise/#complete-data-loading-class","title":"Complete Data Loading Class","text":"<pre><code>class FashionMNISTData:\n    def __init__(self, batch_size=128, val_split=0.1, root=\"./data\", seed=42):\n        self.batch_size = batch_size\n        self.val_split = val_split\n        self.root = root\n        self.seed = seed\n\n        # Step 1 and 2: Load and normalize\n        transform = transforms.ToTensor()\n        full_train = datasets.FashionMNIST(root=self.root, train=True, transform=transform, download=True)\n        test = datasets.FashionMNIST(root=self.root, train=False, transform=transform, download=True)\n\n        # Step 3: Split train/val\n        total_train = len(full_train)\n        val_size = int(total_train * val_split)\n        train_size = total_train - val_size\n        torch.manual_seed(seed)\n        train, val = random_split(full_train, [train_size, val_size])\n\n        # Dataloaders\n        self.train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n        self.val_loader = DataLoader(val, batch_size=batch_size, shuffle=False)\n        self.test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)\n</code></pre>"},{"location":"vae/vae-exercise/#model-implementation","title":"Model Implementation","text":"<p>Now, we define the Variational Autoencoder architecture used to learn the compressed representations of the dataset. </p>"},{"location":"vae/vae-exercise/#1-encoder-and-decoder","title":"1. Encoder and Decoder","text":"<p>The model has 2 parts:</p> <ul> <li> <p>Encoder: a multi-layer perceptron with ReLU (changed to <code>LeakyReLU</code> later) activations that compresses the input image into 2 vectors: the mean (\\(\\mu\\)) and the log variance of the latent space (\\(\\log{\\sigma^2}\\)).</p> </li> <li> <p>Decoder: reconstructs the image from the sampled latent vector. </p> </li> </ul>"},{"location":"vae/vae-exercise/#2-reparametrization-trick","title":"2. Reparametrization Trick","text":"<p>To make the sampling differentiable, we can apply the reparametrization trick:</p> \\[ z = \\mu + \\sigma * \\epsilon, \\space where \\space \\epsilon \\sim \\mathcal{N}(0,1) \\] <p>This allows the network to learn through backpropagation while keeping the stochastic nature of \\(z\\). </p>"},{"location":"vae/vae-exercise/#3-forward-pass","title":"3. Forward Pass","text":"<p>During the forward pass:</p> <ol> <li> <p>The encoder outputs \\(\\mu\\) and \\(\\log{\\sigma^2}\\)</p> </li> <li> <p>A latent vector \\(z\\) is sampled using the reparametrization trick</p> </li> <li> <p>The decoder reconstructs the image \\(\\hat{x}\\)</p> </li> </ol>"},{"location":"vae/vae-exercise/#vae-implementation","title":"VAE Implementation","text":"<p>In order to implement the VAE, we used Torch to create the model architecture. Here is the complete class implementation.</p> <pre><code>class VAE(nn.Module):\n    def __init__(self, input_channels: int = 1, img_size: int = 28, latent_dim: int = 16, hidden_dims: tuple[int, ...] = (512, 256)):\n        super().__init__()\n        self.img_size = img_size\n        self.input_dim = input_channels * img_size * img_size\n        self.latent_dim = latent_dim\n\n        enc_layers = []\n        prev = self.input_dim\n        for h in hidden_dims:\n            enc_layers += [nn.Linear(prev, h), nn.LeakyReLU(0.2, inplace=True)]\n            prev = h\n        self.encoder = nn.Sequential(*enc_layers)\n        self.fc_mu = nn.Linear(prev, latent_dim)\n        self.fc_logvar = nn.Linear(prev, latent_dim)\n\n        dec_layers = []\n        rev_h = list(hidden_dims)[::-1]\n        prev = latent_dim\n        for h in rev_h:\n            dec_layers += [nn.Linear(prev, h), nn.LeakyReLU(0.2, inplace=True)]\n            prev = h\n        dec_layers += [nn.Linear(prev, self.input_dim)]\n        self.decoder = nn.Sequential(*dec_layers)\n\n    def encode(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        x = x.view(x.size(0), -1)\n        h = self.encoder(x)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n\n    # Reparametrization trick (z = mu + std * eps)\n    @staticmethod\n    def reparametrize(mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + std * eps\n\n    def decode(self, z: torch.Tensor) -&gt; torch.Tensor:\n        x_hat = self.decoder(z)\n        x_hat = x_hat.view(x_hat.size(0), 1, self.img_size, self.img_size)\n        return x_hat\n\n    def forward(self, x: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        mu, logvar = self.encode(x)\n        z = self.reparametrize(mu, logvar)\n        x_hat = self.decode(z)\n        return x_hat, mu, logvar\n</code></pre>"},{"location":"vae/vae-exercise/#training","title":"Training","text":"<p>Now, we can train the VAE on the dataset using a reconstruction term (Binary Cross Entropy with Logits) plus a KL term, while monitoring the loss and generating diagnostic visuals. </p> <p>Setup:</p> <ul> <li> <p>Optimizer: Adam with learning rate <code>5e-4</code></p> </li> <li> <p>Batch Size: 128</p> </li> <li> <p>Latent Dim: 32</p> </li> <li> <p>Device: MPS (for MacBook GPU)</p> </li> <li> <p>Checkpoints: best model by validation loss and snapshots of the final model and first epoch</p> </li> </ul>"},{"location":"vae/vae-exercise/#loss","title":"Loss","text":"<p>We train the model with BCE with logits, as the decoder outputs logits and we only apply Sigmoid for visualization. The reconstruction term is computed as a per sample sum over pixels, then averaged over the batch. Here is the implementation of the loss helper functions:</p> <pre><code>def recon_loss(logits, x):\n    per_elem = F.binary_cross_entropy_with_logits(logits, x, reduction=\"none\")\n    per_sample = per_elem.view(per_elem.size(0), -1).sum(dim=1)\n    return per_sample.mean()\n\ndef vae_loss(xhat, x, mu, logvar, beta=beta):\n    recon = recon_loss(xhat, x)\n    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n    return recon + beta * kl, recon.detach(), kl.detach()\n</code></pre> <p>We initially observed that \\(KL \\approx 0\\) and identical, blurry reconstructions. With some research, we discovered this was a case of posterior collapse, where the network finds a shortcut by perfectly satisfying the KL regularization by ignoring the latent variables, so it does not learn a useful representation. To fix this, we implemented 3 changes:</p> <ol> <li> <p>Re-weighting the loss function: the core problem was an imbalance between the 2 parts of the loss. The KL number was overpowering the reconstruction loss, so the optimizer was incentivized to reduce the large KL term to zero and ignore the small reconstruction loss. To fix it, we replaced the mean reconstruction loss with a sum of the loss accross all pixels. This scaled up the reconstruction term.</p> </li> <li> <p>Beta Warmup: in early stages, the model doesn't know how to use the latent variables, so the KL penalty is too big. To fix that, we implemented a warmup schedule, setting the penalty to 0 for the first few epochs, forcing the model to train as a standard autoencoder and learn how to reconstruct, and then gradually adding more value to Beta, up to 1.</p> </li> <li> <p>Improving numerical stability and gradient flow: the training process had poor gradient flow, so it stopped learning. We made 2 stability improvements:</p> <ol> <li> <p>Using logits for loss: we found out that a sigmoid in the decoder's output layer can cause numerical issues when combined with cross entropy loss. We removed it and used a more stable loss function (BCE with logits) that combines both operations safely.</p> </li> <li> <p>Switching to <code>LeakyReLU</code>: the standard <code>ReLU</code> activation function can die during training and stop learning. We switched to the <code>LeakyReLU</code> function, which allows a small gradient to flow even for negative values, ensuring all neurons remain active. </p> </li> </ol> </li> </ol> <p>After these changes, training returned to a stable flow. </p> <p>Here is the implementation of the Beta Warmup function:</p> <pre><code>def beta_schedule(epoch, warmup_epochs=25, max_beta=1.0):\n    return min (max_beta, epoch / warmup_epochs * max_beta)\n</code></pre>"},{"location":"vae/vae-exercise/#training-and-evaluation-loops","title":"Training and Evaluation loops","text":"<p>2 simple routines were implemented: </p> <ul> <li><code>train_one_epoch</code> updates parameters and returns mean total loss, reconstruction error, and KL:</li> </ul> <pre><code>def train_one_epoch(model, loader, opt, device, beta=beta):\n    model.train()\n    total, total_recon, total_kl, n = 0.0, 0.0, 0.0, 0\n    for x, _ in loader:\n        x = x.to(device)\n        xhat, mu, logvar = model(x)\n        loss, recon, kl = vae_loss(xhat, x, mu, logvar, beta=beta)\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        bs = x.size(0)\n        total += loss.item() * bs\n        total_recon += recon.item() * bs\n        total_kl += kl.item() * bs\n        n += bs\n    return total / n, total_recon / n, total_kl / n\n</code></pre> <ul> <li><code>evaluate</code> computes the same metrics on validation: </li> </ul> <pre><code>@torch.no_grad()\ndef evaluate(model, loader, device, beta=beta):\n    model.eval()\n    total, total_recon, total_kl, n = 0.0, 0.0, 0.0, 0\n    for x, _ in loader:\n        x = x.to(device)\n        xhat, mu, logvar = model(x)\n        loss, recon, kl = vae_loss(xhat, x, mu, logvar, beta=beta)\n        bs = x.size(0)\n        total += loss.item() * bs\n        total_recon += recon.item() * bs\n        total_kl += kl.item() * bs\n        n += bs\n    return total / n, total_recon / n, total_kl / n\n</code></pre>"},{"location":"vae/vae-exercise/#monitoring-and-visualization","title":"Monitoring and Visualization","text":"<p>The training loop saves the model at some checkpoints, and outputs the training loss for some epochs. Here is the training loop:</p> <pre><code>epochs = 50\nbest = float(\"inf\")\n\nfor epoch in range(1, epochs + 1):\n    beta = beta_schedule(epoch)\n    tr_loss, tr_rec, tr_kl = train_one_epoch(model, train_loader, optimizer, device, beta)\n    va_loss, va_rec, va_kl = evaluate(model, val_loader, device, beta)\n\n    if va_loss &lt; best:\n        best = va_loss\n        torch.save(model.state_dict(), save_dir / f\"model/vae_fashion_mnist_best.pth\")\n\n    if epoch == 1:\n        torch.save(model.state_dict(), save_dir / \"model/vae_fashion_mnist_epoch_01.pth\")\n\n    if epoch % 10 == 0 or epoch == 1:\n\n        peek_stats(model, val_loader, device)\n\n        print(\n            f\"Epoch {epoch:02d}: \"\n            f\"Train Loss: {tr_loss:.4f} (Recon: {tr_rec:.4f}, KL: {tr_kl:.4f}) | \"\n            f\"Val Loss: {va_loss:.4f} (Recon: {va_rec:.4f}, KL: {va_kl:.4f})\"\n        )\n\ntorch.save(model.state_dict(), save_dir / \"model/vae_fashion_mnist_final.pth\")\n</code></pre> <p>Moreover, we implemented visualization functions. </p>"},{"location":"vae/vae-exercise/#evaluation","title":"Evaluation","text":"<p>After training for 50 epochs, we evaluated the model to assess reconstruction quality, latent space organization, and generative capability, for both the first epoch and the best epoch. </p> <p>For the quantitative analysis, we measured the performance using the negative evidence lower bound (NELBO) and bits per dimension (BPD). These metrics are standard in probabilistic generative modeling, balancing reconstruction accuracy and regularization:</p> <ul> <li> <p>Reconstruction Loss: measures how well the model can reproduce input pixels. </p> </li> <li> <p>KL Divergence: measures how closely the learned latent distribution aligns with a unit Gaussian, ensuring regularization.</p> </li> <li> <p>NELBO (KL + Reconstruction): the total training objective; lower indicates a better trade-off between accuracy and smooth latent structure. </p> </li> <li> <p>BPD: normalizes NELBO by image size and converts it to bits, making it easier to compare models.</p> </li> </ul> <p>The results we got were, for the first epoch:</p> <ul> <li> <p>Reconstruction Loss: 537.49</p> </li> <li> <p>KL: 5.85</p> </li> <li> <p>NELBO: 543.33</p> </li> <li> <p>BPD: 0.9998</p> </li> </ul> <p>And for the best epoch:</p> <ul> <li> <p>Reconstruction Loss: 516.20</p> </li> <li> <p>KL: 7.96</p> </li> <li> <p>NELBO: 524.16</p> </li> <li> <p>BPD: 0.9645</p> </li> </ul> <p>At the beginning of training, the model produced weaker reconstructions and a nearly inactive latent space, which will be shown later as images. By the best epoch, the KL divergence increased and the reconstruction loss decreased, indicating better image fidelity. </p> <p>The evaluation function we used is as follows:</p> <pre><code>import math\n\n@torch.no_grad()\ndef evaluate_metrics(model, loader, device, img_size=28):\n    model.eval()\n    total_recon, total_kl, n = 0.0, 0.0, 0\n    for x, _ in loader:\n        x = x.to(device)\n        logits, mu, logvar = model(x)\n        per_elem = F.binary_cross_entropy_with_logits(logits, x, reduction=\"none\")\n        recon = per_elem.view(per_elem.size(0), -1).sum(dim=1).mean()\n        kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n\n        bs = x.size(0)\n        total_recon += recon.item() * bs\n        total_kl += kl.item() * bs\n        n += bs\n\n    recon_avg = total_recon / n\n    kl_avg = total_kl / n\n    nelbo = recon_avg + kl_avg\n    bpd = nelbo / (img_size * img_size * math.log(2))\n\n    return {\"recon\": recon_avg, \"kl\": kl_avg, \"nelbo\": nelbo, \"bpd\": bpd}\n</code></pre>"},{"location":"vae/vae-exercise/#visualization","title":"Visualization","text":"<p>This section summarizes the qualitative results using reconstructions, prior samples, latent space projections and latent manipulations. Again, we have images for both the first epoch and the best epoch, in order to compare. </p>"},{"location":"vae/vae-exercise/#1-reconstructions-input-output","title":"1. Reconstructions (input -&gt; output)","text":"<p>Best Epoch:</p> <p></p> <p>First Epoch:</p> <p></p> <p>This shows how well the model evolved in reconstructing images. At the first epoch, reconstructions are blurry and look similar to each other, while at the best epoch, they are much sharper and more accurate.</p> <pre><code>@torch.no_grad()\ndef show_reconstructions(model, loader, device, n=8, save_path=None):\n    model.eval()\n    x, _ = next(iter(loader))\n    x = x[:n].to(device)\n    logits, _, _ = model(x)\n    xhat = torch.sigmoid(logits)\n    x = x.cpu().numpy()\n    xhat = xhat.cpu().numpy()\n    fig, axes = plt.subplots(2, n, figsize=(n * 2, 4))\n    for i in range(n):\n        axes[0, i].imshow(x[i, 0], cmap=\"gray\")\n        axes[0, i].axis(\"off\")\n        axes[1, i].imshow(xhat[i, 0], cmap=\"gray\")\n        axes[1, i].axis(\"off\")\n    axes[0,0].set_ylabel(\"Input\", fontsize=16)\n    axes[1,0].set_ylabel(\"Reconstruction\", fontsize=16)\n    if save_path:\n        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n    plt.show()\n</code></pre>"},{"location":"vae/vae-exercise/#2-prior-samples-unconditioned-generation","title":"2. Prior Samples (unconditioned generation)","text":"<p>Best Epoch:</p> <p></p> <p>First Epoch:</p> <p></p> <p>This shows samples generated from the prior distribution. At the first epoch, samples are mostly noise, while at the best epoch, they resemble realistic clothing items.</p> <pre><code>@torch.no_grad()\ndef show_prior_samples(model, device, n=16, save_path=None):\n    model.eval()\n    z = torch.randn(n, model.latent_dim).to(device)\n    logits = model.decode(z)\n    xhat = torch.sigmoid(logits).cpu().numpy()\n    cols = int(n**0.5)\n    rows = (n + cols - 1) // cols\n    fig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 2))\n    axes = axes.ravel()\n    for i in range(n):\n        axes[i].imshow(xhat[i, 0], cmap=\"gray\")\n        axes[i].axis(\"off\")\n    if save_path:\n        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n    plt.show()\n</code></pre>"},{"location":"vae/vae-exercise/#3-latent-space-scatter-2d-projection-of-latent-codes","title":"3. Latent Space Scatter (2D projection of latent codes)","text":"<p>Best Epoch (t-SNE):</p> <p></p> <p>First Epoch (t-SNE):</p> <p></p> <p>Best Epoch (PCA):</p> <p></p> <p>First Epoch (PCA):</p> <p></p> <p>This projection shows how the latent space is organized. At the first epoch, points are clustered together without clear separation, while at the best epoch, some distinct clusters emerge corresponding to different clothing categories.</p> <pre><code>@torch.no_grad()\ndef latent_scatter(model, loader, device, n=3000, method=\"pca\", save_path=None):\n    model.eval()\n    Xs, Ys = [], []\n    seen = 0\n    for x, y in loader:\n        x = x.to(device)\n        _, mu, _ = model(x)\n        Xs.append(mu.cpu())\n        Ys.append(y)\n        seen += x.size(0)\n        if seen &gt;= n: break\n\n    Z = torch.cat(Xs, dim=0)[:n].numpy()\n    Y = torch.cat(Ys, dim=0)[:n].numpy()\n\n    if method == \"tsne\":\n        try:\n            Z2 = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\").fit_transform(Z)\n        except Exception:\n            method = \"pca\"\n    if method == \"pca\":\n        Zt = torch.from_numpy(Z)\n        Zt = Zt - Zt.mean(0, keepdim=True)\n        U, S, V = torch.pca_lowrank(Zt, q=2)\n        Z2 = (Zt @ V[:, :2]).numpy()\n\n    plt.figure(figsize=(5,5))\n    plt.scatter(Z2[:,0], Z2[:,1], c=Y, s=6, cmap=\"tab10\")\n    plt.axis(\"off\")\n    plt.title(\"Latent Space (\u03bc) \u2013 \" + method.upper())\n    if save_path: \n        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n    plt.show()\n</code></pre>"},{"location":"vae/vae-exercise/#4-latent-space-interpolation","title":"4. Latent Space Interpolation","text":"<p>Best Epoch:</p> <p></p> <p>First Epoch:</p> <p></p> <p>The linear interpolation between validation images.</p> <pre><code>@torch.no_grad()\ndef interpolate_between(model, loader, device, steps=10, save_path=None):\n    model.eval()\n    x, y = next(iter(loader))\n    a, b = x[0:1].to(device), x[1:2].to(device)\n    mu_a, _ = model.encode(a)\n    mu_b, _ = model.encode(b)\n    alphas = torch.linspace(0, 1, steps, device=device).unsqueeze(1)\n    z = (1 - alphas) * mu_a + alphas * mu_b\n    logits = model.decode(z)\n    imgs = torch.sigmoid(logits).cpu().numpy()\n    fig, axes = plt.subplots(1, steps, figsize=(steps*1.2, 1.2))\n    for i in range(steps):\n        axes[i].imshow(imgs[i, 0], cmap=\"gray\")\n        axes[i].axis(\"off\")\n    fig.suptitle(\"Latent Interpolation\", y=0.95)\n    if save_path: \n        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n    plt.show()\n</code></pre>"},{"location":"vae/vae-exercise/#latent-traversals","title":"Latent Traversals","text":"<p>Best Epoch:</p> <p></p> <p>First Epoch:</p> <p></p> <p>This shows how varying individual latent dimensions affects the generated images. At the first epoch, changes are minimal and noisy, while at the best epoch, distinct features like sleeve length and clothing type emerge.</p> <pre><code>@torch.no_grad()\ndef latent_traversal(model, loader, device, dim=0, span=3.0, steps=11, save_path=None):\n    model.eval()\n    x, _ = next(iter(loader))\n    x0 = x[0:1].to(device)\n    mu, _ = model.encode(x0)\n    z = mu.repeat(steps, 1)\n    alphas = torch.linspace(-span, span, steps, device=device)\n    z[:, dim] = alphas\n    logits = model.decode(z)\n    imgs = torch.sigmoid(logits).cpu().numpy()\n    fig, axes = plt.subplots(1, steps, figsize=(steps*1.2, 1.2))\n    for i in range(steps):\n        axes[i].imshow(imgs[i, 0], cmap=\"gray\")\n        axes[i].axis(\"off\")\n    fig.suptitle(f\"Latent Traversal \u2014 dim {dim}\", y=0.95)\n    if save_path: \n        plt.savefig(save_path, bbox_inches=\"tight\", dpi=200)\n    plt.show()\n</code></pre>"},{"location":"vae/vae-exercise/#qualitative-summary","title":"Qualitative Summary","text":"<p>The reconstructions sharpen over training, and prior samples become more coherent and diverse. The scatter plots show clearer class separation at the best epoch, indicating more structured latent representations. The interpolations are smoother and more meaningful, and the latent traversals reveal interpretable changes in the generated images.</p>"},{"location":"vae/vae-exercise/#conclusion","title":"Conclusion","text":"<p>In this exercise we implemented a Variational Autoencoder from scratch using Pytorch, trained it on the Fashion MNIST dataset, and evaluated its performance both quantitatively and qualitatively.</p>"},{"location":"vae/vae-exercise/#summary-of-findings","title":"Summary of Findings","text":"<p>Throughout training, the model gradually improved both reconstruction quality and latent space organization. The quantitative metrics show that images became sharper and faithful to the inputs, the KL divergence shows that latent variables were effectively utilized, and the BPD indicates efficient compression.</p> <p>Visually, reconstructions evolved from blurry and homogeneus to distinct and more detailed. Latent projections revealed emerging clusters by clothing type, confirming the model learned meaningful representations. Interpolations and traversals demonstrated smooth transitions and interpretable features, indicating a well-structured latent space.</p>"},{"location":"vae/vae-exercise/#challenges-and-insights","title":"Challenges and Insights","text":"<p>Training the VAE was challenging due to issues like loss scaling and posterior collapse. We mitigated this by using the methods described in the Training section, which stabilized learning and encouraged effective use of latent variables. </p> <p>Key Insights:</p> <ul> <li> <p>A properly tuned beta scheduler is crucial for preventing collapse and encouraging a useful latent space.</p> </li> <li> <p>Even a simple fully connected VAE without convolutions can capture useful and interpretable representations when trained carefully.</p> </li> <li> <p>The learned latent space is smooth and continuous, enabling interpolation and controlled image variation. </p> </li> </ul> <p>Note: Artificial intelligence was used in this exercise for code completion and review, for translating math formulas to Latex, and for text review.</p>"}]}